varImp <- paste((length(dataImp[1,])-1), " variable")
} else {
varImp <- paste((length(dataImp[1,])-1), " variables")
}
knitr::opts_chunk$set(echo = FALSE)
set.seed(123)
dataImpTrain <- sample_n(ortoDataImp, round(obsImp*0.67))
dataImpTest <- subset(ortoDataImp, !(ortoDataImp$octamer %in% dataImpTrain$octamer))
dataImpTestLabels <- dataImpTest$state
dataImpTest$state <- NULL
dataImpTrainLabels <- dataImpTrain$state
dataImpTrain$state <- NULL
knitr::opts_chunk$set(echo = FALSE)
head(ortoDataImp, 20)
head(ortoDataSch, 20)
head(dataImpTest)
head(dataImpTrain)
dataImpTrain
dataImpTrainLabels
head(dataImp)
knitr::opts_chunk$set(echo = FALSE)
## Carga de librerías con funciones necesarias para el proyecto
require(dplyr)
require(class)
## Carga de datos no convirtiendo strings a factores, definiendo nombres de columna:
dataImp <- read.csv(file = "D:\\MU_Bioinformatica\\Machine Learning\\PEC1\\impensData.txt", header = FALSE, stringsAsFactors = FALSE, col.names = c("octamer","state"));
dataSch <- read.csv(file = "D:\\MU_Bioinformatica\\Machine Learning\\PEC1\\schillingData.txt", header = FALSE, stringsAsFactors = FALSE, col.names = c("octamer","state"));
## DEFINICIÓN DE FUNCIONES:
## ------------------------
## Diccionario de aminoácidos a octámeros. Crea vectores de tantos ceros como aminoácidos diferentes haya en los datos y sustituye por un 1 una posición dada por la posición del aminoácido en la lista de aminoácidos diferentes, y elabora un diccionario con estos valores:
aminoDict <- function(data) {
## Desactivar notación cientifica
options(scipen=999)
## Dataframe vacío para almacenar los aminoácidos y sus vectores ortogonales
result <- data.frame(aminoacid=character(), value=double(), stringsAsFactors = FALSE)
## String concatenado de aminoácidos
octamerString <- paste(data$octamer, collapse = "")
## Aminoácidos presentes:
aminoacids <- as.vector(dimnames(table(strsplit(octamerString, "")))[[1]])
aminoCount <- length(aminoacids)
##Línea del aminoácido en el diccionario.
for (letter in aminoacids) {
aminoPos <- which(aminoacids == letter)
orthoAmino <- as.vector(rep(0, aminoCount))
orthoAmino[aminoPos] <- 1
orthoAmino <- paste(orthoAmino, collapse = "")
orthoAminoLine <- c (letter, orthoAmino)
## Añadir la línea al final del dataframe diccionario
result[aminoPos,] <- orthoAminoLine
}
## Reactivar notación científica
options(scipen=0)
return(result)
}
## Generación
## Funcion 1 - codificación de un octámero a vector ortogonal.
single_octamer_encode <- function(oct,dictionary){
oct <- substring(oct, seq(1,nchar(oct),1), seq(1,nchar(oct),1))
resultVector <- c()
for (letter in oct) {
aminoValue <- dictionary$value[which(dictionary$aminoacid == letter)]
aminoValueVector <- as.numeric(substring(aminoValue, seq(1,nchar(aminoValue),1), seq(1,nchar(aminoValue),1)))
resultVector <- c(resultVector, aminoValueVector)
}
result <- resultVector
return(result)
}
## Función 2 - codificación de todos los octámeros de la muestra a vectores ortogonales
data_octamer_encode <- function(data,dictionary) {
octamers <- c(data$octamer)
## Desactivar notación cientifica
options(scipen=999)
## Dataframe vacío para almacenar los octámeros y sus vectores ortogonales
##  result <- data.frame(octamer=character(), orthovalue=numeric(), stringsAsFactors = FALSE)
result <- data.frame(matrix(, nrow=0, ncol=160))
for (octamer in octamers) {
octamerPos <- which(data$octamer == octamer)
octamerValue <- single_octamer_encode(octamer,dictionary)
##    octamerVector <- c(octamer,octamerValue)
result[octamerPos,] <- octamerValue
}
result$octamer <- data$octamer
result$state <- data$state
return(result)
}
## Generación del diccionario de aminoácidos correspondiente a la muestra
aminoDictionaryImp <- aminoDict(dataImp)
aminoDictionarySch <- aminoDict(dataSch)
## Generación del diccionario de aminoácidos correspondiente a la muestra
ortoDataImp <- data_octamer_encode(dataImp,aminoDictionaryImp)
ortoDataSch <- data_octamer_encode(dataSch,aminoDictionarySch)
aminoDictionaryImp
knitr::opts_chunk$set(echo = FALSE)
dataFile <- "D:\\MU_Bioinformatica\\Machine Learning\\PEC1\\impensData.txt"
dataFileAlias <- "impensData.txt"
knitr::opts_chunk$set(echo = FALSE)
dataImp <- read.csv(file = dataFile, header = FALSE, stringsAsFactors = FALSE, col.names = c("octamer","state"))
dataImp$state <- factor(dataImp$state, levels = c("-1","1"), labels = c("non-cleaved","cleaved"))
ortoDataImp$state <- factor(ortoDataImp$state, levels = c("-1","1"), labels = c("non-cleaved","cleaved"))
obsImp <- length(dataImp$octamer)
if ((length(dataImp[1,])-1) == 1) {
varImp <- paste((length(dataImp[1,])-1), " variable")
} else {
varImp <- paste((length(dataImp[1,])-1), " variables")
}
knitr::opts_chunk$set(echo = FALSE)
set.seed(123)
dataImpTrain <- sample_n(ortoDataImp, round(obsImp*0.67))
dataImpTest <- subset(ortoDataImp, !(ortoDataImp$octamer %in% dataImpTrain$octamer))
dataImpTestLabels <- dataImpTest$state
dataImpTest$state <- NULL
dataImpTrainLabels <- dataImpTrain$state
dataImpTrain$state <- NULL
knitr::opts_chunk$set(echo = FALSE)
head(ortoDataImp, 20)
head(ortoDataSch, 20)
dataImpTrainLabels
head(dataImpTestLabels)
head(ortoDataImp$octamer)
dataImpPred <- knn(train = dataImpTrain, test = dataImpTest, cl = dataImpTrainLabels, k = 3)
head(dataImpTrain)
dataImpPred <- knn(train = dataImpTrain[,-161], test = dataImpTest[,-161], cl = dataImpTrainLabels, k = 3)
dataImpPred
head(dataImpPred,20)
head(dataImpTestLabels,20)
head(dataImpTest[,-"octamer"])
head(dataImpTest[,-dataImpTest$octamer])
head(dataImpTest[,-which(names(dataImpTest) == "octamer"))
head(dataImpTest[,-which(names(dataImpTest) == "octamer")])
knitr::opts_chunk$set(echo = FALSE)
dataImpPred3 <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 3)
dataImpPred5 <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 5)
dataImpPred7 <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 7)
dataImpPred11 <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 11)
install.packages("gmodels")
require(gmodels)
knitr::opts_chunk$set(echo = FALSE)
CrossTable(x = dataImpTestLabels, y = dataImpPred3, prop.chisq = FALSE)
knitr::opts_chunk$set(echo = FALSE)
CrossTable(x = dataImpTestLabels, y = dataImpPred5, prop.chisq = FALSE)
knitr::opts_chunk$set(echo = FALSE)
CrossTable(x = dataImpTestLabels, y = dataImpPred7, prop.chisq = FALSE)
knitr::opts_chunk$set(echo = FALSE)
CrossTable(x = dataImpTestLabels, y = dataImpPred11, prop.chisq = FALSE)
head(ortoDataImp, 1)
head(ortoDataSch, 1)
knitr::opts_chunk$set(echo = FALSE)
kvalue <- round(sqrt(obsImp))
dataImpPredK <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = kvalue)
CrossTable(x = dataImpTestLabels, y = dataImpPredK, prop.chisq = FALSE)
head(ortoDataImp, 1)
head(ortoDataSch, 1)
kvalue
dataImpPredK
dataImpPredK <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 20)
dataImpPredK
dataImpPredK <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 13)
CrossTable(x = dataImpTestLabels, y = dataImpPredK, prop.chisq = FALSE)
dataImpPredK <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 15)
CrossTable(x = dataImpTestLabels, y = dataImpPredK, prop.chisq = FALSE)
dataImpPredK <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 14)
CrossTable(x = dataImpTestLabels, y = dataImpPredK, prop.chisq = FALSE)
dataImpPredK <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = 13)
CrossTable(x = dataImpTestLabels, y = dataImpPredK, prop.chisq = FALSE)
knitr::opts_chunk$set(echo = FALSE)
kvalue <- 13
dataImpPredK <- knn(train = dataImpTrain[,-which(names(dataImpTest) == "octamer")], test = dataImpTest[,-which(names(dataImpTest) == "octamer")], cl = dataImpTrainLabels, k = kvalue)
CrossTable(x = dataImpTestLabels, y = dataImpPredK, prop.chisq = FALSE)
head(ortoDataImp, 1)
head(ortoDataSch, 1)
knitr::opts_chunk$set(echo = TRUE, fig.lp = "Figura ")
##Carga de librerías necesarias
require(stats)
require(neuralnet)
require(nnet)
require(caret)
require(kernlab)
require(e1071)
dataPath <- file.path("D:\\MU_Bioinformatica\\Machine Learning\\PEC3\\data.csv")
classPath <- file.path("D:\\MU_Bioinformatica\\Machine Learning\\PEC3\\class.csv")
tumorData <- read.csv(dataPath)
tumorClass <- as.vector(read.csv(classPath))
td.levels = c('EWS','BL','NB','RMS')
##PCA con datos centrados y escalados
td.pca <- prcomp(tumorData, center = TRUE, scale. = TRUE)
##Lista de las 10 componentes principales que más varianza explican
td.pcs <- as.vector(colnames(td.pca$x)[1:10])
##Literal human-readable de las 10 componentes principales
td.pcsString <- paste(paste0(td.pcs[1:9], collapse=", "), td.pcs[10], sep=" y ")
##Resumen de los datos de la PCA
summary(td.pca)
tumorDataPC <- as.data.frame(subset(td.pca$x, select=td.pcs))
##Definición de la función para normalizar los datos
normalizer <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
##Aplicación de la función de normalización sobre los datos
tumorDataPCnorm <- as.data.frame(lapply(tumorDataPC, normalizer))
##Conversión de la variable factor a un booleano de pertenencia a una clase de tumor
tumorDataPCnormNum <- cbind(tumorDataPCnorm, class.ind(as.factor(tumorClass[,1])))
##Asignación de etiquetas a los niveles
colnames(tumorDataPCnormNum)[11:14] <- td.levels
##Añadido de una columna numérica con los niveles del factor Clase de Tumor, para uso en las comprobaciones K-fold Validation
tumorDataPCnormFact <- tumorDataPCnorm
tumorDataPCnormFact$tumorLevel <- as.numeric(tumorClass[,1])
tumorDataPCnormFact$tumorLevel <- factor(tumorDataPCnormFact$tumorLevel, labels = c("EWS", "BL", "NB", "RMS"))
##Establecer la semilla para hacer repetibles todos los procedimientos aleatorios
set.seed(12345)
##Establecer el número de columnas que se separarán para training, siendo el 67% redondeado hacia arriba.
training_nrows <- ceiling(length(tumorDataPCnormNum[,1])*0.67)
##Establecer el número de columnas que se separarán para test, siendo el 33% redondeado hacia abajo.
test_nrows <- floor(length(tumorDataPCnormNum[,1])*0.33)
##subset de datos para training
training_set <- tumorDataPCnormNum[sample(nrow(tumorDataPCnormNum), training_nrows), ]
test_set <- subset(tumorDataPCnormNum, !(row.names(tumorDataPCnormNum) %in% row.names(training_set)))
##Descripción de una fórmula de modelo que utilice el nivel de tumor como variable diana y las 10 PCs como predictores
##ANNformula <- paste(paste(td.levels, collapse = " + "), paste(td.pcs, collapse = " + "), sep = " ~ ")
ANNformula <- paste(paste(td.levels, collapse = " + "), paste(td.pcs, collapse = " + "), sep = " ~ ")
##Generacion de una red neural de 1 nodo con los datos de training, usando como variable diana tumorLevel
##y como variables predictoras todas las PC. Para clasificación se utiliza la opción de linear.output = FALSE.
ANN_1 <- neuralnet(ANNformula, data = training_set, hidden = 1, linear.output=FALSE)
##Generacion de una red neural de 1 nodo con los datos de training, usando como variable diana tumorLevel
##y como variables predictoras todas las PC. Para clasificación se utiliza la opción de linear.output = FALSE.
ANN_3 <- neuralnet(ANNformula, data = training_set, hidden = 3, linear.output=FALSE)
##Generacion de una red neural de 1 nodo con los datos de training, usando como variable diana tumorLevel
##y como variables predictoras todas las PC. Para clasificación se utiliza la opción de linear.output = FALSE.
ANN_10 <- neuralnet(ANNformula, data = training_set, hidden = 10, linear.output=FALSE)
##Teniendo las dos redes neurales, computar resultados y comprobar comportamiento de la predicción
ANN_1.comp <- compute(ANN_1, test_set[1:10])
ANN_3.comp <- compute(ANN_3, test_set[1:10])
ANN_10.comp <- compute(ANN_10, test_set[1:10])
##Extraer resultados del cómputo
ANN_1.res <- ANN_1.comp$net.result
ANN_3.res <- ANN_3.comp$net.result
ANN_10.res <- ANN_10.comp$net.result
cor(round(ANN_1.res),test_set[,11:14])
cor(round(ANN_3.res),test_set[,11:14])
cor(round(ANN_10.res),test_set[,11:14])
table(round(ANN_1.res[,1]),test_set[,11])
table(round(ANN_1.res[,2]),test_set[,11])
table(round(ANN_1.res[,3]),test_set[,11])
table(round(ANN_1.res[,4]),test_set[,11])
##Establecer la semilla para hacer repetibles todos los procedimientos aleatorios
set.seed(12345)
##Establecer el número de columnas que se separarán para training, siendo el 67% redondeado hacia arriba.
training_nrowsFact <- ceiling(length(tumorDataPCnormFact[,1])*0.67)
##Establecer el número de columnas que se separarán para test, siendo el 33% redondeado hacia abajo.
test_nrowsFact <- floor(length(tumorDataPCnormFact[,1])*0.33)
##subset de datos para training
training_setFact <- tumorDataPCnormFact[sample(nrow(tumorDataPCnormFact), training_nrowsFact), ]
test_setFact <- subset(tumorDataPCnormFact, !(row.names(tumorDataPCnormFact) %in% row.names(training_setFact)))
##Definimos los parámetros de control para el modelo.
ANN.ctrl <- trainControl(method = "cv", number=3, savePred=T)
#my.grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(5, 6, 7))
ANN.tune <- train(tumorLevel~., data=training_setFact, method = "nnet", trControl = ANN.ctrl)
##Generación de una SVM que utilice como diana el tipo de cáncer, y como predictores todas las demás variables mediante un kernel lineal.
SVM_lin <- ksvm(tumorLevel ~ ., data = training_setFact, kernel = "vanilladot")
##Generación de una SVM que utilice como diana el tipo de cáncer, y como predictores todas las demás variables mediante un kernel RBF.
SVM_rbf <- ksvm(tumorLevel ~ ., data = training_setFact, kernel = "rbfdot")
##Elaboración de predicciones con la SVM con kernel lineal
SVM_lin.pred <- predict(SVM_lin, test_setFact[,1:10])
##Elaboración de predicciones con la SVM con kernel RBF
SVM_rbf.pred <- predict(SVM_rbf, test_setFact[,1:10])
SVM_lin.ctrl <- trainControl(method = "cv", savePred=T, classProb=T)
SVM_1.tune <- train(tumorLevel~., data=training_setFact, method = "svmLinear", trControl = SVM_lin.ctrl)
head(SVM_1.tune$pred)
head(SVM_1.tune$pred$pred)
table(SVM_1.tune$pred$pred,SVM_1.tune$pred$obs)
ANN_1
ANN_1.res
knitr::opts_chunk$set(echo = TRUE, fig.lp = "Figura ")
##Carga de librerías necesarias
require(stats)
require(neuralnet)
require(nnet)
require(caret)
require(kernlab)
require(e1071)
dataPath <- file.path("D:\\MU_Bioinformatica\\Machine Learning\\PEC3\\data.csv")
classPath <- file.path("D:\\MU_Bioinformatica\\Machine Learning\\PEC3\\class.csv")
tumorData <- read.csv(dataPath)
tumorClass <- as.vector(read.csv(classPath))
td.levels = c('EWS','BL','NB','RMS')
##PCA con datos centrados y escalados
td.pca <- prcomp(tumorData, center = TRUE, scale. = TRUE)
##Lista de las 10 componentes principales que más varianza explican
td.pcs <- as.vector(colnames(td.pca$x)[1:10])
##Literal human-readable de las 10 componentes principales
td.pcsString <- paste(paste0(td.pcs[1:9], collapse=", "), td.pcs[10], sep=" y ")
##Resumen de los datos de la PCA
summary(td.pca)
tumorDataPC <- as.data.frame(subset(td.pca$x, select=td.pcs))
##Definición de la función para normalizar los datos
normalizer <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
##Aplicación de la función de normalización sobre los datos
tumorDataPCnorm <- as.data.frame(lapply(tumorDataPC, normalizer))
##Conversión de la variable factor a un booleano de pertenencia a una clase de tumor
tumorDataPCnormNum <- cbind(tumorDataPCnorm, class.ind(as.factor(tumorClass[,1])))
##Asignación de etiquetas a los niveles
colnames(tumorDataPCnormNum)[11:14] <- td.levels
##Añadido de una columna numérica con los niveles del factor Clase de Tumor, para uso en las comprobaciones K-fold Validation
tumorDataPCnormFact <- tumorDataPCnorm
tumorDataPCnormFact$tumorLevel <- as.numeric(tumorClass[,1])
tumorDataPCnormFact$tumorLevel <- factor(tumorDataPCnormFact$tumorLevel, labels = c("EWS", "BL", "NB", "RMS"))
##Establecer la semilla para hacer repetibles todos los procedimientos aleatorios
set.seed(12345)
##Establecer el número de columnas que se separarán para training, siendo el 67% redondeado hacia arriba.
training_nrows <- ceiling(length(tumorDataPCnormNum[,1])*0.67)
##Establecer el número de columnas que se separarán para test, siendo el 33% redondeado hacia abajo.
test_nrows <- floor(length(tumorDataPCnormNum[,1])*0.33)
##subset de datos para training
training_set <- tumorDataPCnormNum[sample(nrow(tumorDataPCnormNum), training_nrows), ]
test_set <- subset(tumorDataPCnormNum, !(row.names(tumorDataPCnormNum) %in% row.names(training_set)))
##Descripción de una fórmula de modelo que utilice el nivel de tumor como variable diana y las 10 PCs como predictores
##ANNformula <- paste(paste(td.levels, collapse = " + "), paste(td.pcs, collapse = " + "), sep = " ~ ")
ANNformula <- paste(paste(td.levels, collapse = " + "), paste(td.pcs, collapse = " + "), sep = " ~ ")
##Generacion de una red neural de 1 nodo con los datos de training, usando como variable diana tumorLevel
##y como variables predictoras todas las PC. Para clasificación se utiliza la opción de linear.output = FALSE.
ANN_1 <- neuralnet(ANNformula, data = training_set, hidden = 1, linear.output=FALSE)
##Generacion de una red neural de 1 nodo con los datos de training, usando como variable diana tumorLevel
##y como variables predictoras todas las PC. Para clasificación se utiliza la opción de linear.output = FALSE.
ANN_3 <- neuralnet(ANNformula, data = training_set, hidden = 3, linear.output=FALSE)
##Generacion de una red neural de 1 nodo con los datos de training, usando como variable diana tumorLevel
##y como variables predictoras todas las PC. Para clasificación se utiliza la opción de linear.output = FALSE.
ANN_10 <- neuralnet(ANNformula, data = training_set, hidden = 10, linear.output=FALSE)
##Teniendo las dos redes neurales, computar resultados y comprobar comportamiento de la predicción
ANN_1.comp <- compute(ANN_1, test_set[1:10])
ANN_3.comp <- compute(ANN_3, test_set[1:10])
ANN_10.comp <- compute(ANN_10, test_set[1:10])
##Extraer resultados del cómputo
ANN_1.res <- ANN_1.comp$net.result
ANN_3.res <- ANN_3.comp$net.result
ANN_10.res <- ANN_10.comp$net.result
cor(round(ANN_1.res),test_set[,11:14])
cor(round(ANN_3.res),test_set[,11:14])
cor(round(ANN_10.res),test_set[,11:14])
table(round(ANN_1.res[,1]),test_set[,11])
table(round(ANN_1.res[,2]),test_set[,11])
table(round(ANN_1.res[,3]),test_set[,11])
table(round(ANN_1.res[,4]),test_set[,11])
##Establecer la semilla para hacer repetibles todos los procedimientos aleatorios
set.seed(12345)
##Establecer el número de columnas que se separarán para training, siendo el 67% redondeado hacia arriba.
training_nrowsFact <- ceiling(length(tumorDataPCnormFact[,1])*0.67)
##Establecer el número de columnas que se separarán para test, siendo el 33% redondeado hacia abajo.
test_nrowsFact <- floor(length(tumorDataPCnormFact[,1])*0.33)
##subset de datos para training
training_setFact <- tumorDataPCnormFact[sample(nrow(tumorDataPCnormFact), training_nrowsFact), ]
test_setFact <- subset(tumorDataPCnormFact, !(row.names(tumorDataPCnormFact) %in% row.names(training_setFact)))
##Definimos los parámetros de control para el modelo.
ANN.ctrl <- trainControl(method = "cv", number=3, savePred=T)
#my.grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(5, 6, 7))
ANN.tune <- train(tumorLevel~., data=training_setFact, method = "nnet", trControl = ANN.ctrl)
##Generación de una SVM que utilice como diana el tipo de cáncer, y como predictores todas las demás variables mediante un kernel lineal.
SVM_lin <- ksvm(tumorLevel ~ ., data = training_setFact, kernel = "vanilladot")
##Generación de una SVM que utilice como diana el tipo de cáncer, y como predictores todas las demás variables mediante un kernel RBF.
SVM_rbf <- ksvm(tumorLevel ~ ., data = training_setFact, kernel = "rbfdot")
##Elaboración de predicciones con la SVM con kernel lineal
SVM_lin.pred <- predict(SVM_lin, test_setFact[,1:10])
##Elaboración de predicciones con la SVM con kernel RBF
SVM_rbf.pred <- predict(SVM_rbf, test_setFact[,1:10])
SVM_lin.ctrl <- trainControl(method = "cv", savePred=T, classProb=T)
SVM_1.tune <- train(tumorLevel~., data=training_setFact, method = "svmLinear", trControl = SVM_lin.ctrl)
Us1 <- c(3,,4,7,2)
Us1 <- c(3,4,7,2)
Us2 <- c(5,9,1,3)
mean(Us1)
mean(Us2)
mean(sum(Us1,Us2))
sum(Us1,Us2)
sum(Us1,Us2)/4
mean(mean(3,5),mean(4,9),mean(7,1),mean(2,3))
x <- 1:20
y <- x+rnorm(20)
polinomial_lm <- lm(y ~ x + I(x^2))
x_matrix <- cbind(1, x, x^2)
head(x_matrix)
polinomial_direct_calc <- solve(t(X) %*% X) %*% t(X) %*% y
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
cbind(polinomial_lm_calc, polinomial_direct_calc)
polinomial_orders <- seq(3:10)
for (i in 1:polinomial_orders){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
cbind(polinomial_lm_calc, polinomial_direct_calc)
}
polinomial_orders <- seq(3:10)
for (i in (1:polinomial_orders)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
cbind(polinomial_lm_calc, polinomial_direct_calc)
}
polinomial_orders <- seq(3:10)
for (i in polinomial_orders){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
cbind(polinomial_lm_calc, polinomial_direct_calc)
}
polinomial_orders <- seq(3:10)
for (i in (3:10)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
cbind(polinomial_lm_calc, polinomial_direct_calc)
}
polinomial_orders <- seq(3:10)
for (i in (3)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
cbind(polinomial_lm_calc, polinomial_direct_calc)
}
polinomial_orders <- seq(3:10)
for (i in (3:3)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
cbind(polinomial_lm_calc, polinomial_direct_calc)
}
polinomial_orders <- seq(3:10)
for (i in (3:3)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_orders <- seq(3:10)
for (i in (3:5)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_orders <- seq(3:10)
for (i in (3:9)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_orders <- seq(3:10)
for (i in (3:7)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_orders <- seq(3:10)
for (i in (3:6)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_orders <- seq(3:10)
for (i in polinomial_orders){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_orders <- seq(3:6)
for (i in polinomial_orders){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_max <- 6
for (i in (1:polinomial_max)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_max <- 6
for (i in (3:polinomial_max)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
polinomial_max <- 6
for (i in (3:10)){
polinomial_lm <- lm(y ~ x + I(x^i))
x_matrix <- cbind(1, x, x^i)
polinomial_direct_calc <- solve(t(x_matrix) %*% x_matrix) %*% t(x_matrix) %*% y
polinomial_lm_calc <- polinomial_lm$coefficients
print(cbind(polinomial_lm_calc, polinomial_direct_calc))
}
setwd("D:/MU_Bioinformatica/TFM/REPORT")
