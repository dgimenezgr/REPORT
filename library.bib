Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Mass2017,
author = {Brian, Author and Venables, Bill and Bates, Douglas M and Firth, David and Ripley, Maintainer Brian},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/MASS.pdf:pdf},
title = {{Package ‘ MASS '}},
year = {2017}
}
@book{HarryHHarman1976,
abstract = {This thoroughly revised third edition of Harry H. Harman's authoritative text incorporates the many new advances made in computer science and technology over the last ten years. The author gives full coverage to both theoretical and applied aspects of factor analysis from its foundations through the most advanced techniques. This highly readable text will be welcomed by researchers and students working in psychology, statistics, economics, and related disciplines.},
author = {{Harry H. Harman}},
edition = {Illustrate},
editor = {{University of Chicago Press}, 1976},
isbn = {0226316521, 9780226316529},
pages = {269},
title = {{Modern Factor Analysis}},
year = {1976}
}
@article{VanDerMaaten2009,
abstract = {In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear tech- niques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identi- fying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.},
author = {{Van Der Maaten}, Laurens and Postma, Eric and {Van Den Herik}, Jaap},
doi = {10.1080/13506280444000102},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/dimensionality{\_}reduction{\_}a{\_}comparative{\_}review.pdf:pdf},
issn = {0169328X},
journal = {October},
pages = {1--35},
pmid = {7877450},
title = {{Dimensionality Reduction : A Comparative Review}},
url = {http://www.uvt.nl/ticc},
year = {2009}
}
@misc{chollet2015,
author = {et al Chollet, Fran{\c{c}}ois},
booktitle = {GitHub},
title = {{Keras}},
url = {https://keras.io/getting-started/faq/},
urldate = {2017-10-24},
year = {2015}
}
@article{Hanley1982,
abstract = {A representation and interpretation of the area under a receiver operating characteristic (ROC) curve obtained by the "rating" method, or by mathematical predictions based on patient characteristics, is presented. It is shown that in such a setting the area represents the probability that a randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a randomly chosen non-diseased subject. Moreover, this probability of a correct ranking is the same quantity that is estimated by the already well-studied nonparametric Wilcoxon statistic. These two relationships are exploited to (a) provide rapid closed-form expressions for the approximate magnitude of the sampling variability, i.e., standard error that one uses to accompany the area under a smoothed ROC curve, (b) guide in determining the size of the sample required to provide a sufficiently reliable estimate of this area, and (c) determine how large sample sizes should be to ensure that one can statistically detect differences in the accuracy of diagnostic techniques.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Hanley, A.J. and McNeil, J.B.},
doi = {10.1148/radiology.143.1.7063747},
eprint = {NIHMS150003},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/meaning{\_}and{\_}use{\_}of{\_}the{\_}area{\_}under{\_}a{\_}ROC.pdf:pdf},
isbn = {0033-8419 (Print) 0033-8419 (Linking)},
issn = {0033-8419},
journal = {Radiology},
pages = {29--36},
pmid = {7063747},
title = {{The Meaning and Use of the Area under a Receiver Operating Characteristic (ROC) Curve}},
url = {http://radiology.rsna.org/content/143/1/29.full.pdf},
volume = {143},
year = {1982}
}
@article{Ramentol2012,
abstract = {Imbalanced data is a common problem in classification. This phenomenon is growing in importance since it appears in most real domains. It has special relevance to highly imbalanced data-sets (when the ratio between classes is high). Many techniques have been developed to tackle the problem of imbalanced training sets in supervised learning. Such techniques have been divided into two large groups: those at the algorithm level and those at the data level. Data level groups that have been emphasized are those that try to balance the training sets by reducing the larger class through the elimination of samples or increasing the smaller one by constructing new samples, known as undersampling and oversampling, respectively. This paper proposes a new hybrid method for preprocessing imbalanced data-sets through the construction of new samples, using the Synthetic Minority Oversampling Technique together with the application of an editing technique based on the Rough Set Theory and the lower approximation of a subset. The proposed method has been validated by an experimental study showing good results using C4.5 as the learning algorithm.},
author = {Ramentol, Enislay and Caballero, Yail{\'{e}} and Bello, Rafael and Herrera, Francisco},
doi = {10.1007/s10115-011-0465-6},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/SMOTE-hybrid.pdf:pdf},
isbn = {1011501104},
issn = {02191377},
journal = {Knowledge and Information Systems},
keywords = {Classification,Data preparation,Imbalanced data-sets,Oversampling,Rough sets theory,Undersampling},
number = {2},
pages = {245--265},
title = {{SMOTE-RSB *: A hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory}},
volume = {33},
year = {2012}
}
@article{Tipping1999,
abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss the advantages conveyed by the definition of a probability density function for PCA.},
author = {Tipping, Michael E. and Bishop, Christopher M.},
doi = {10.1111/1467-9868.00196},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tipping, Bishop - 1999 - Probabilistic Principal Component Analysis.pdf:pdf},
isbn = {9781424444427},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
number = {3},
pages = {611--622},
pmid = {21092268},
title = {{Probabilistic Principal Component Analysis}},
url = {http://doi.wiley.com/10.1111/1467-9868.00196},
volume = {61},
year = {1999}
}
@article{Hyvarinen2000,
abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of nongaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Hyv{\"{a}}rinen, Aapo and Oja, Erkki},
doi = {10.1016/S0893-6080(00)00026-5},
eprint = {1504.05070},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/ICA{\_}Hyvarinen.pdf:pdf},
isbn = {3589451327},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {Independent component analysis,blind signal separation,factor analysis,projection pursuit,representation,source separation},
number = {45},
pages = {411--430},
pmid = {10946390},
title = {{Independent Component Analysis: Algorithms and Applications}},
volume = {13},
year = {2000}
}
@article{Maaten2008,
abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare itwith many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {Maaten, Laurens Van Der and Hinton, Geoffrey},
doi = {10.1007/s10479-011-0841-3},
eprint = {1307.1662},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/visualizing data using tsne IMPORTANTE.pdf:pdf},
isbn = {1532-4435},
issn = {1940-6029},
journal = {Journal of Machine Learning Research 1},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
number = {1},
pages = {267--84},
pmid = {20652508},
title = {{Visualizing Data using t-SNE}},
url = {http://link.springer.com/10.1007/s10479-011-0841-3{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/20652508},
volume = {620},
year = {2008}
}
@article{Bigorra2017,
author = {Bigorra, Laura and Merino, Anna and Alf{\'{e}}rez, Santiago and Rodellar, Jos{\'{e}}},
doi = {10.1002/jcla.22024},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/FeatureAnalysis{\_}2016.pdf:pdf},
issn = {10982825},
journal = {Journal of Clinical Laboratory Analysis},
keywords = {cytology,hematology,image processing,leukemia,pathology},
number = {2},
pages = {1--9},
title = {{Feature Analysis and Automatic Identification of Leukemic Lineage Blast Cells and Reactive Lymphoid Cells from Peripheral Blood Cell Images}},
volume = {31},
year = {2017}
}
@book{Hutchison2013,
author = {Hutchison, David and Mitchell, John C},
doi = {10.1007/978-3-642-36973-5},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/interpretation{\_}of{\_}precision{\_}recall{\_}and{\_}F-Score.pdf:pdf},
isbn = {978-3-642-36972-8},
pages = {357--374},
title = {{Advances in Information Retrieval}},
url = {http://link.springer.com/10.1007/978-3-540-35488-8{\_}13},
volume = {7814},
year = {2013}
}
@misc{Scipy2001,
author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu},
title = {{SciPy: Open source scientific tools for Python}},
url = {http://www.scipy.org/},
urldate = {2017-10-24},
year = {2001}
}
@book{StanleyAMulaik2009,
abstract = {Providing a practical, thorough understanding of how factor analysis works, Foundations of Factor Analysis, Second Edition discusses the assumptions underlying the equations and procedures of this method. It also explains the options in commercial computer programs for performing factor analysis and structural equation modeling. This long-awaited edition takes into account the various developments that have occurred since the publication of the original edition.},
author = {{Stanley A Mulaik}},
edition = {2nd, Illus},
editor = {{CRC Press}, 2009},
isbn = {1420099817, 9781420099812},
pages = {548},
title = {{Foundations of Factor Analysis, Second Edition}},
year = {2009}
}
@article{Zhang2013,
abstract = {In automated remote sensing based image analysis, it is important to consider the multiple features of a certain pixel, such as the spectral signature, morphological property, and shape feature, in both the spatial and spectral domains, to improve the classification accuracy. Therefore, it is essential to consider the complementary properties of the different features and combine them in order to obtain an accurate classification rate. In this paper, we introduce a modified stochastic neighbor embedding (MSNE) algorithm for multiple features dimension reduction (DR) under a probability preserving projection framework. For each feature, a probability distribution is constructed based on t-distributed stochastic neighbor embedding (. t-SNE), and we then alternately solve t-SNE and learn the optimal combination coefficients for different features in the proposed multiple features DR optimization. Compared with conventional remote sensing image DR strategies, the suggested algorithm utilizes both the spatial and spectral features of a pixel to achieve a physically meaningful low-dimensional feature representation for the subsequent classification, by automatically learning a combination coefficient for each feature. The classification results using hyperspectral remote sensing images (HSI) show that MSNE can effectively improve RS image classification performance. {\textcopyright} 2013 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Zhang, Lefei and Zhang, Liangpei and Tao, Dacheng and Huang, Xin},
doi = {10.1016/j.isprsjprs.2013.05.009},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/A modified stochastic neighbor embedding for multi-feature dimension.pdf:pdf},
isbn = {0924-2716},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Classification,Dimension reduction,Hyperspectral image,Multiple features,Stochastic neighbor embedding},
pages = {30--39},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{A modified stochastic neighbor embedding for multi-feature dimension reduction of remote sensing images}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2013.05.009},
volume = {83},
year = {2013}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/scikit-learn{\_}machine-learning-in-python.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Riffenburgh1957,
author = {Riffenburgh, Robert Harry},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/lda-thesis.pdf:pdf},
number = {May},
title = {{B.S., M.s.}},
year = {1957}
}
@article{Tsne2016,
author = {Package, Type},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/tsne.pdf:pdf},
pages = {2--5},
title = {{Package ‘ tsne '}},
year = {2016}
}
@book{Tryfos1998,
abstract = {Focusing on the principal methods for analysis and forecasting, the text is designed to fit two types of course design, the traditional approach for a technical course or the frequently used approach in business courses. Examples, exercises, problems and small and large cases are provided to fit into either or both approaches. An instructor's manual is available which includes solutions, author's notes of each case and possible alternatives, and the programs used by the author.},
author = {Gorsuch, Richard L.},
edition = {English},
editor = {{Wiley; 1 edition (January 23}, 1998)},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Example - 2001 - Chapter 14 Factor analysis.pdf:pdf},
isbn = {978-0471123842},
pages = {592},
title = {{Factor analysis}},
year = {1998}
}
@article{Puigvi2017,
author = {Puigv{\'{i}}, L and Merino, A and Alf{\'{e}}rez, S and Acevedo, A and Rodellar, J},
doi = {10.1136/jclinpath-2017-204389},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/NewQuantitative{\_}2017.pdf:pdf},
issn = {14724146},
journal = {J Clin Pathol},
pages = {Published Online First: 13 June 2017},
title = {{New quantitative features for the morphological differentiation of abnormal lymphoid cell images from peripheral blood}},
year = {2017}
}
@book{Chun2006,
author = {Chun, By Wesley J and Chun, Wesley J},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Core Python Programming, Second Edition (2006).pdf:pdf},
isbn = {9780132269933},
keywords = {{\#} ISBN-10: 0132269937,{\#} ISBN-13: 978-0132269933,{\#} Language: English,{\#} Paperback: 1120 pages,{\#} Paperback: 1120 pages$\backslash$r$\backslash$n{\#} Publisher: Prentice H,{\#} Publisher: Prentice Hall PTR,2006),2006)$\backslash$r$\backslash$n{\#} Language: Engl,2nd edition (September 18},
pages = {1--1120},
title = {{Core Python Programming , Second Edition}},
year = {2006}
}
@article{Chen2017,
abstract = {Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
doi = {10.1016/j.eswa.2016.10.065},
eprint = {1404.7828},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/deep{\_}learning{\_}in{\_}neural{\_}networks{\_}an{\_}overview.pdf:pdf},
isbn = {0925-2312},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deep neural network,Natural language processing,Sentiment analysis},
pages = {221--230},
pmid = {19932002},
title = {{Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN}},
volume = {72},
year = {2017}
}
@article{Klecka1980,
abstract = {Linear Discriminant Analysis (LDA) is a well-known scheme for feature extraction and dimension reduction. It has been used widely in many ap-plications involving high-dimensional data, such as face recognition and image retrieval. An intrinsic limitation of classical LDA is the so-called singularity problem, that is, it fails when all scatter matrices are singu-lar. A well-known approach to deal with the singularity problem is to apply an intermediate dimension reduction stage using Principal Com-ponent Analysis (PCA) before LDA. The algorithm, called PCA+LDA, is used widely in face recognition. However, PCA+LDA has high costs in time and space, due to the need for an eigen-decomposition involving the scatter matrices. In this paper, we propose a novel LDA algorithm, namely 2DLDA, which stands for 2-Dimensional Linear Discriminant Analysis. 2DLDA over-comes the singularity problem implicitly, while achieving efficiency. The key difference between 2DLDA and classical LDA lies in the model for data representation. Classical LDA works with vectorized representa-tions of data, while the 2DLDA algorithm works with data in matrix representation. To further reduce the dimension by 2DLDA, the combi-nation of 2DLDA and classical LDA, namely 2DLDA+LDA, is studied, where LDA is preceded by 2DLDA. The proposed algorithms are ap-plied on face recognition and compared with PCA+LDA. Experiments show that 2DLDA and 2DLDA+LDA achieve competitive recognition accuracy, while being much more efficient.},
author = {Klecka, William},
doi = {10.4135/9781412983938},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/2547-two-dimensional-linear-discriminant-analysis.pdf:pdf},
isbn = {9780803914919},
issn = {0034-6543},
journal = {Advances in Neural Information Processing Systems},
number = {60},
pages = {1569--1576},
title = {{Discriminant Analysis}},
url = {http://methods.sagepub.com/book/discriminant-analysis},
volume = {17},
year = {1980}
}
@misc{ufldltutorial,
author = {University, Stanford},
title = {{Unsupervised Feature Learning and Deep Learning Tutorial}},
url = {ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/}
}
@article{Rifai2011,
abstract = {Although the structure and composition of plant communities is known to influence the functioning of ecosystems, there is as yet no agreement as to how these should be described from a functional perspective. We tested the biomass ratio hypothesis, which postulates that ecosystem properties should depend on species traits and on species contribution to the total biomass of the community, in a successional sere following vineyard abandonment in the Mediterranean region of France. Ecosystem-specific net primary productivity, litter decomposition rate, and total soil carbon and nitrogen varied significantly with field age, and correlated with community-aggregated (i.e., weighed according to the relative abundance of species) functional leaf traits. The three easily measurable traits tested, specific leaf area, leaf dry matter content, and nitrogen concentration, provide a simple means to scale up from organ to ecosystem functioning in complex plant communities. We propose that they be called {\&}8220;functional markers,{\&}8221; and be used to assess the impacts of community changes on ecosystem properties induced, in particular, by global change drivers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rifai, Salah and Muller, Xavier},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/contractive{\_}autoencoders{\_}feature{\_}extraction.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {1467-9280},
journal = {Icml},
number = {1},
pages = {833--840},
pmid = {25052830},
title = {{Contractive Auto-Encoders : Explicit Invariance During Feature Extraction}},
url = {http://www.icml-2011.org/papers/455{\_}icmlpaper.pdf},
volume = {85},
year = {2011}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Hyvarinen97.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Independent Component Analysis by Minimization of Mutual Information}},
volume = {53},
year = {2013}
}
@article{Leslie2002,
abstract = {We introduce a new sequence-similarity kernel, the spectrum kernel, for use with support vector machines (SVMs) in a discriminative approach to the protein classification problem. Our kernel is conceptually simple and efficient to compute and, in experiments on the SCOP database, performs well in comparison with state-of-the-art methods for homology detection. Moreover, our method produces an SVM classifier that allows linear time classification of test sequences. Our experiments provide evidence that string-based kernels, in conjunction with SVMs, could offer a viable and computationally efficient alternative to other methods of protein classification and homology detection.},
author = {Leslie, Christina and Eskin, Eleazar and Noble, William Stafford},
doi = {10.1142/9789812799623_0053},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/SVM-protein-classification.pdf:pdf},
isbn = {1793-5091 (Print)},
issn = {2335-6936},
journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
pages = {564--575},
pmid = {11928508},
title = {{The spectrum kernel: a string kernel for SVM protein classification.}},
volume = {575},
year = {2002}
}
@misc{R2008,
address = {Vienna},
author = {Team, R Development Core},
doi = {3-900051-07-0},
publisher = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {http://www.r-project.org},
urldate = {2017-10-23},
year = {2008}
}
@article{Ballard1987,
abstract = {In the development of large-scale knowledge networks , much recent progress has been inspired by connections to neurobiology. An important component of any" neural " network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must ... $\backslash$n},
author = {Ballard, Dana H.},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/modular{\_}learning{\_}in{\_}neural{\_}networks.pdf:pdf},
isbn = {preprint},
journal = {Aaai},
pages = {279--284},
title = {{Modular Learning in Neural Networks}},
year = {1987}
}
@article{Barandela2004,
abstract = {The problem of imbalanced training sets in supervised pattern recognition methods is receiving growing attention. Imbalanced training sample means that one class is represented by a large number of examples while the other is represented by only a few. It has been observed that this situation, which arises in several practical domains, may produce an important deterioration of the classification accuracy, in particular with patterns belonging to the less represented classes. In this paper we present a study concerning the relative merits of several re-sizing techniques for handling the imbalance issue. We assess also the convenience of combining some of these techniques.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Barandela, Ricardo and Valdovinos, Rosa M and S{\'{a}}nchez, J Salvador and Ferri, Francesc J},
doi = {10.1007/b98738},
eprint = {9780201398298},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/The{\_}Imbalanced{\_}Training{\_}Sample{\_}Problem{\_}U.pdf:pdf},
isbn = {3540225706},
issn = {03029743},
journal = {Structural, Syntactic, and Statistical Pattern Recognition},
pages = {806--814},
pmid = {4520227},
title = {{The Imbalanced Training Sample Problem : Under or over Sampling ?}},
year = {2004}
}
@article{Hinton2002,
abstract = {Abstract We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high- ... $\backslash$n},
author = {Hinton, Geoffrey E and Roweis, Sam T},
doi = {http://books.nips.cc/papers/files/nips15/AA45.pdf},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/2276-stochastic-neighbor-embedding.pdf:pdf},
isbn = {0262025507},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {833--840},
title = {{Stochastic neighbor embedding}},
year = {2002}
}
@article{Hyvarinen2001,
abstract = {A comprehensive introduction to ICA for students and practitioners Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more. Independent Component Analysis is divided into four sections that cover: General mathematical concepts utilized in the book The basic ICA model and its solution Various extensions of the basic ICA model Real-world applications for ICA models Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.},
author = {Hyv{\"{a}}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
doi = {10.1002/0471221317},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/bookfinal{\_}ICA.pdf:pdf},
isbn = {047140540X},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
number = {1},
pages = {135--144},
pmid = {20421937},
title = {{Independent Component Analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520306000509{\%}5Cnhttp://doi.wiley.com/10.1002/0471221317},
volume = {21},
year = {2001}
}
@article{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jolliffe, I T},
doi = {10.2307/1270093},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jolliffe - 2002 - Principal Component Analysis, Second Edition.pdf:pdf},
isbn = {0387954422},
issn = {00401706},
journal = {Encyclopedia of Statistics in Behavioral Science},
number = {3},
pages = {487},
pmid = {21435900},
title = {{Principal Component Analysis, Second Edition}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/0470013192.bsa501/full},
volume = {30},
year = {2002}
}
@article{Stone2005,
abstract = {Given a set ofM signal mixtures (x1,x2, . . . ,xM) (e.g. microphone outputs), each of which is a different mixture of a set of M statistically independent source signals (s1, s2, . . . , sM) (e.g. voices), independent component analysis (ICA) recovers the source signals (voices) from the signal mixtures. ICA is based on the assumptions that source signals are statistically independent and that they have non-Gaussian distributions. Different physical processes usually generate statistically independent and non-Gaussian signals, so that, in the process of extracting such signals from a set of signal mixtures, ICA effectively recovers the underlying physical causes for a given set of measured signal mixtures.},
author = {Stone, James V.},
doi = {10.1002/0470013192.bsa297},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/ica{\_}encyc{\_}jvs4everrit2005.pdf:pdf},
isbn = {9780470860809},
journal = {Encyclopedia of Statistics in Behavioral Science},
keywords = {Independent Component Analysis},
number = {Novembre},
pages = {1--11},
title = {{Independent Component Analysis}},
url = {http://doi.wiley.com/10.1002/0470013192.bsa297},
volume = {2},
year = {2005}
}
@article{Naugler2014,
abstract = {Peripheral blood smear image examination is a part of the routine work of every laboratory. The manual examination of these images is tedious, time-consuming and suffers from interobserver variation. This has motivated researchers to develop different algorithms and methods to automate peripheral blood smear image analysis. Image analysis itself consists of a sequence of steps consisting of image segmentation, features extraction and selection and pattern classification. The image segmentation step addresses the problem of extraction of the object or region of interest from the complicated peripheral blood smear image. Support vector machine (SVM) and artificial neural networks (ANNs) are two common approaches to image segmentation. Features extraction and selection aims to derive descriptive characteristics of the extracted object, which are similar within the same object class and different between different objects. This will facilitate the last step of the image analysis process: pattern classification. The goal of pattern classification is to assign a class to the selected features from a group of known classes. There are two types of classifier learning algorithms: supervised and unsupervised. Supervised learning algorithms predict the class of the object under test using training data of known classes. The training data have a predefined label for every class and the learning algorithm can utilize this data to predict the class of a test object. Unsupervised learning algorithms use unlabeled training data and divide them into groups using similarity measurements. Unsupervised learning algorithms predict the group to which a new test object belong to, based on the training data without giving an explicit class to that object. ANN, SVM, decision tree and K-nearest neighbor are possible approaches to classification algorithms. Increased discrimination may be obtained by combining several classifiers together.},
author = {Naugler, Christopher and Mohammed, EmadA and Mohamed, MostafaM. A. and Far, BehrouzH},
doi = {10.4103/2153-3539.129442},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/naugler2014.pdf:pdf},
issn = {2153-3539},
journal = {Journal of Pathology Informatics},
keywords = {access this article online,feature extraction,feature selection,microscopic image analysis,peripheral blood smear,segmentation},
number = {1},
pages = {9},
pmid = {24843821},
title = {{Peripheral blood smear image analysis: A comprehensive review}},
url = {http://www.jpathinformatics.org/text.asp?2014/5/1/9/129442},
volume = {5},
year = {2014}
}
@article{Burges2010,
abstract = {We give a tutorial overview of several foundational methods for dimension reduction. We divide the methods into projective methods and methods that model the manifold on which the data lies. For projective methods, we review projection pursuit, principal component analysis (PCA), kernel PCA, probabilistic PCA, canonical correlation analysis (CCA), kernel CCA, Fisher discriminant analysis, oriented PCA, and several techniques for sufficient dimension reduction. For the manifold methods, we review multidimensional scaling (MDS), landmark MDS, Isomap, locally linear embedding, Laplacian eigenmaps, and spectral clustering. Although the review focuses on foundations, we also provide pointers to some more modern techniques. We also describe the correlation dimension as one method for estimating the intrinsic dimension, and we point out that the notion of dimension can be a scale-dependent quantity. The Nystr{\"{o}}m method, which links several of the manifold algorithms, is also reviewed. We use a publicly available dataset to illustrate some of the methods. The goal is to provide a self-contained overview of key concepts underlying many of these algorithms, and to give pointers for further reading.},
author = {Burges, Cjc},
doi = {10.1561/2200000002},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/dimension{\_}reduction{\_}a{\_}guided{\_}tour.pdf:pdf},
isbn = {9781601983787},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
number = {4},
pages = {275--365},
title = {{Dimension reduction: A guided tour}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL{\&}doi=2200000002{\%}5Cnhttp://research.microsoft.com/pubs/150728/FnT{\_}dimensionReduction.pdf},
volume = {2},
year = {2010}
}
@article{Garrett-Mayer2006,
abstract = {Slide set},
author = {Garrett-Mayer, Elizabeth},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garrett-Mayer - 2006 - Statistics in Psychosocial Lecture 8 Factor Analysis.pdf:pdf},
journal = {The Johns Hopkins University},
pages = {1--56},
title = {{Statistics in Psychosocial Lecture 8: Factor Analysis}},
year = {2006}
}
@article{Tucker1951,
author = {Tucker, Ledyard R.},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/tucker-19514.pdf:pdf},
journal = {Personnel Research Section Report},
title = {{A method for synthesis of factor analysis studies}},
volume = {984},
year = {1951}
}
@unpublished{RcppDL2015,
author = {Package, Type and Kou, Author Qiang and Sugomori, Yusuke},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/RcppDL.pdf:pdf},
title = {{Package ‘ RcppDL '}},
year = {2015}
}
@article{Yonelinas2007,
abstract = {Receiver operating characteristic (ROC) analysis is being used increasingly to examine the memory processes underlying recognition memory. The authors discuss the methodological issues involved in conducting and analyzing ROC results, describe the various models that have been developed to account for these results, review the behavioral empirical literature, and assess the models in light of those results. The empirical literature includes studies of item recognition, relational recognition (e.g., source and associative tests), as well as exclusion and remember-know tasks. Nine empirical regularities are described, and a number of unresolved empirical issues are identified. The results indicate that several common classes of recognition models, such as pure threshold and pure signal detection models, are inadequate to account for recognition memory, whereas several hybrid models that incorporate a signal detection-based process and a threshold recollection or attention process are in better agreement with the results. The results indicate that there are at least 2 functionally distinct component/processes underlying recognition memory. In addition, the ROC results have various implications for how recognition memory performance should be measured.},
author = {Yonelinas, Andrew P. and Parks, Colleen M.},
doi = {10.1037/0033-2909.133.5.800},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Receiver{\_}operating{\_}characteristics{\_}ROCs{\_}recognition.pdf:pdf},
isbn = {0033-2909 (Print)},
issn = {1939-1455},
journal = {Psychological Bulletin},
keywords = {dual,know,receiver operating characteristics,recognition,remember,signal detection theory},
number = {5},
pages = {800--832},
pmid = {17723031},
title = {{Receiver operating characteristics (ROCs) in recognition memory: A review.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.133.5.800},
volume = {133},
year = {2007}
}
@article{Luengo2009,
author = {Luengo, {\AE} J {\AE}},
doi = {10.1007/s00500-008-0392-y},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luengo - 2009 - A study of statistical techniques and performance measures for genetics-based machine learning accuracy and interpretab.pdf:pdf},
keywords = {genetic algorithms {\'{a}} statistical,genetics-based machine learning {\'{a}},s kappa {\'{a}} interpretability,tests {\'{a}} cohen,tests {\'{a}} non-parametric,{\'{a}} classification},
pages = {959--977},
title = {{A study of statistical techniques and performance measures for genetics-based machine learning : accuracy and interpretability}},
year = {2009}
}
@article{Fodor2002,
abstract = {this paper, we assume that we have n observations, each being a realization of the p- dimensional random variable x = (x 1 , . . . , x p )    with mean E(x) =  = ( 1 , . . . ,  p )    and covariance matrix E{\{}(x    )(x      = {\#} pp . We denote such an observation matrix by X =    i,j : 1      p, 1      n{\}}. If  i and {\#} i =    {\#} (i,i) denote the mean and the standard deviation of the ith random variable, respectively, then we will often standardize the observations x i,j by (x i,j      i )/  {\#} i , where   i =  x i = 1/n    j=1 x i,j , and  {\#} i = 1/n    j=1 (x i,j     x i )},
author = {Fodor, Imola},
doi = {10.1.1.8.5098},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/a{\_}survey{\_}of{\_}dimension{\_}reduction{\_}techniques.pdf:pdf},
title = {{A Survey of Dimension Reduction Techniques}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.5098},
year = {2002}
}
@article{Halko2009,
abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, speed, and robustness. These claims are supported by extensive numerical experiments and a detailed error analysis.},
archivePrefix = {arXiv},
arxivId = {0909.4061},
author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
doi = {10.1137/090771806},
eprint = {0909.4061},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Halko-et-al-2009.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
keywords = {dimension reduction,eigenvalue decomposition,interpolative decomposition,johnson,lindenstrauss lemma,matrix approximation,parallel algorithm,pass-efficient algorithm,principal component analysis,random matrix,randomized algorithm,rank-revealing qr factoriza-,singular value decomposition,streaming algorithm,tion},
pages = {1--74},
title = {{Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}},
url = {http://arxiv.org/abs/0909.4061},
year = {2009}
}
@article{Joachims1999,
abstract = {Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Joachims, Thorsten and Dortmund, Universitat and Joachimscsuni-dortmundde, Thorsten},
doi = {10.1109/ICEMI.2009.5274151},
eprint = {arXiv:1301.3781v3},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/SVM-large{\_}scale.pdf:pdf},
isbn = {9781424438631},
issn = {15279995},
journal = {Advances in Kernel Methods - Support Vector Learning},
pages = {41--56},
pmid = {18244602},
title = {{Making Large-Scale SVM Learning Practical}},
url = {http://svmlight.joachims.org/{\%}5Cnhttps://eldorado.uni-dortmund.de/handle/2003/2596},
year = {1999}
}
@article{Bunte2012,
abstract = {We present a systematic approach to the mathematical treatment of the t-distributed stochastic neighbor embedding (t-SNE) and the stochastic neighbor embedding (SNE) method. This allows an easy adaptation of the methods or exchange of their respective modules. In particular, the divergence which measures the difference between probability distributions in the original and the embedding space can be treated independently from other components like, e.g. the similarity of data points or the data distribution. We focus on the extension for different divergences and propose a general framework based on the consideration of Fr{\'{e}}chet-derivatives. This way the general approach can be adapted to the user specific needs. {\textcopyright} 2012 Elsevier B.V.},
author = {Bunte, Kerstin and Haase, Sven and Biehl, Michael and Villmann, Thomas},
doi = {10.1016/j.neucom.2012.02.034},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/stochastic-neighbor-embedding-for-dimension-reduction.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Dimension reduction,Divergence optimization,Nonlinear embedding,Stochastic neighbor embedding,Visualization},
pages = {23--45},
publisher = {Elsevier},
title = {{Stochastic neighbor embedding (SNE) for dimension reduction and visualization using arbitrary divergences}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.02.034},
volume = {90},
year = {2012}
}
@article{Chen2006,
abstract = {This article investigates the performance of combining support vector machines (SVM) and various feature selection strategies. Some of them are ﬁlter- type approaches: general feature selection methods independent of SVM, and some are wrapper-type methods: modiﬁcations of SVM which can be used to select fea- tures. We apply these strategies while participating at NIPS 2003 Feature Selection Challenge and rank third as a group.},
author = {Chen, Yi-wei and Lin, Chih-jen},
doi = {10.1007/978-3-540-35488-8_13},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/f-score.pdf:pdf},
isbn = {9783540354871},
issn = {14349922},
journal = {Feature Extraction},
number = {1},
pages = {315--324},
title = {{Combining SVMs with Various Feature Selection Strategies}},
url = {http://link.springer.com/10.1007/978-3-540-35488-8{\_}13},
volume = {324},
year = {2006}
}
@article{Walt2011,
author = {Walt, Stefan Van Der and Colbert, S Chris and Varoquaux, Ga{\"{e}}l and Walt, Stefan Van Der and Colbert, S Chris and Varoquaux, Ga{\"{e}}l and Numpy, The},
doi = {10.1109/MCSE.2011.37},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/numpy{\_}final.pdf:pdf},
title = {{The NumPy array: a structure for efficient numerical computation}},
url = {https://hal.inria.fr/inria-00564007/document},
year = {2011}
}
@article{Bengio2013,
abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).},
archivePrefix = {arXiv},
arxivId = {1305.6663},
author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
eprint = {1305.6663},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/generalized-denoising-auto-encoders-as-generative-models.pdf:pdf},
isbn = {10495258},
issn = {10495258},
pages = {1--9},
title = {{Generalized Denoising Auto-Encoders as Generative Models}},
url = {http://arxiv.org/abs/1305.6663},
year = {2013}
}
@article{Pet2010,
author = {Petˇ, M},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/WDS10{\_}113{\_}i2{\_}Petricek.pdf:pdf},
isbn = {9788073781392},
pages = {82--87},
title = {{Components in Data Analysis}},
volume = {1},
year = {2010}
}
@article{Bro2014,
abstract = {{\textless}p{\textgreater}Principal component analysis is one of the most important and powerful methods in chemometrics as well as in a wealth of other areas.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bro, Rasmus and Smilde, Age K.},
doi = {10.1039/C3AY41907J},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bro, Smilde - 2014 - Principal component analysis.pdf:pdf},
isbn = {1939-0068},
issn = {1759-9660},
journal = {Anal. Methods},
number = {9},
pages = {2812--2831},
pmid = {20931840},
title = {{Principal component analysis}},
url = {http://xlink.rsc.org/?DOI=C3AY41907J},
volume = {6},
year = {2014}
}
@article{Guyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/an{\_}introduction{\_}to{\_}variable{\_}selection.pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Nosrati2011,
abstract = {in this paper, we are going to represent an introduction to Python programming language and prove it as a suitable language for both learning and real world programming. Due to this, we will begin with philosophy and history of this language, and then get into its features. Program types will be investigated as the next step, and then alternatives and complements of Python which are some useful packages and modules will be introduced. As the last part, some of important software and websites that are written in Python will be introduces, to be a real world evidence for Python.},
author = {Nosrati, Masoud},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/python{\_}an{\_}appropriate{\_}language{\_}for{\_}real{\_}world{\_}programming.pdf:pdf},
journal = {World Applied Programming},
keywords = {Key word},
number = {12},
pages = {110--117},
title = {{Python: An appropriate language for real world programming}},
year = {2011}
}
@article{Benattar2001,
abstract = {Distinguishing leukemic phases of B-cell disorders in peripheral blood smears is well recognized to be difficult in some cases since it depends on subtle and subjective criteria. In order to quantify cytological features and to assess objective descriptions, a morphometric analysis was performed on 83 peripheral blood smears of B-cells disorders (n = 77) and healthy donors (n = 6). Using standardized May-Grunwald Giemsa staining, standardized image acquisition system and well defined microscopic fields, we have analyzed lymphoid cells, measuring morphometric and color parameters. By combining seven relevant morphometric criteria (the nuclear shape, the cellular shape and area, the nucleo-cytoplasmic ratio, the nuclear red/blue ratio, the cytoplasmic green/blue ratio and the proportion of cells with nucleolus), we have established a score that could range from a minimum of -3 (large B-CLL type) to a maximum of +8 (large MCL type): negative scores corresponds to different types of B-CLL (n = 30), including “atypical B-CLL” (n = 6), the score zero correspond to healthy donors (n = 6) used as baseline, the positive score values correspond to +1 for Follicular lymphoma (n = 2), +3 for Splenic Lymphoma with Villous Lymphocytes (n = 12), +4 for Hairy Cell Leukemia (n = 7), +5 for Hairy Cell Leukemia-variant (n = 2), +6 for B-prolymphocytic leukemia (n = 6) and +7 and +8 for most Mantle Cell Lymphoma (n = 18). Testing T-cell disorders samples (n = 10) using the same protocol, the profile is different and cannot be confused with B-cell diseases.Our scoring system indicates that measurement of some common morphologic features in standardized conditions provides objective criteria to characterize those diseases and might be helpful for diagnosis.},
author = {Benattar, Laurence and Flandrin, Georges},
doi = {10.3109/10428190109097674},
journal = {Leukemia {\&} Lymphoma},
number = {1-2},
pages = {29--40},
pmid = {11699219},
title = {{Morphometric and Colorimetric Analysis of Peripheral Blood Smears Lymphocytes in B-Cell Disorders: Proposal for a Scoring System}},
url = {http://dx.doi.org/10.3109/10428190109097674},
volume = {42},
year = {2001}
}
@article{Smith2002,
abstract = {This tutorial is designed to give the reader an understanding of Principal Components Analysis (PCA). PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. Before getting to a description of PCA, this tutorial first introduces mathematical concepts that will be used in PCA. It covers standard deviation, covariance, eigenvec- tors and eigenvalues. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar. There are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. If further information is required, the mathematics textbook Elementary Linear Algebra 5e by Howard Anton, Publisher JohnWiley {\&} Sons Inc, ISBN 0-471-85223-6 is a good source of information regarding the mathematical back- ground. 1},
archivePrefix = {arXiv},
arxivId = {1511.06448},
author = {Smith, Lindsay I},
doi = {10.1080/03610928808829796},
eprint = {1511.06448},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith - 2002 - A tutorial on Principal Components Analysis Introduction.pdf:pdf},
isbn = {0471852236},
issn = {03610926},
journal = {Statistics},
keywords = {analysis,utorial on principal components},
pages = {52},
pmid = {16765218},
title = {{A tutorial on Principal Components Analysis Introduction}},
url = {http://www.mendeley.com/research/computational-genome-analysis-an-introduction-statistics-for-biology-and-health/},
volume = {51},
year = {2002}
}
@article{Ng2011,
abstract = {Beschreibt Neuronale Netze, Backpropagation und (Sparse) Auto-Encoders},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Ng, Andrew},
doi = {10.1371/journal.pone.0006098},
eprint = {arXiv:1506.03733v1},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Sparse{\_}autoencoder.pdf:pdf},
isbn = {1595937935},
issn = {19326203},
journal = {CS294A Lecture notes},
pages = {1--19},
pmid = {19568420},
title = {{Sparse autoencoder}},
url = {http://www.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf},
year = {2011}
}
@article{Farrelly,
author = {Farrelly, Colleen M},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/reviewofmethodsfordimensionreduction.pdf:pdf},
title = {{Review of Methods for Dimension Reduction Reasons to Reduce}}
}
@article{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/3048-greedy-layer-wise-training-of-deep-networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {153},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
volume = {19},
year = {2007}
}
@article{Vincent2008,
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6550v4},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1145/1390156.1390294},
eprint = {arXiv:1412.6550v4},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/extracting{\_}and{\_}compsoing{\_}robust{\_}features{\_}autoencoders.pdf:pdf},
isbn = {9781605582054},
issn = {1605582050},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
number = {July},
pages = {1096--1103},
pmid = {15540460},
title = {{Extracting and composing robust features with denoising autoencoders}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390294},
year = {2008}
}
@article{Sokolova2006,
abstract = {Different evaluation measures assess different characteristics of machine learning algorithms. The empirical evaluation of algorithms and classifiers is a matter of on-going debate between researchers. Although most measures in use today focus on a classifier's ability to identify classes correctly, we suggest that, in certain cases, other properties, such as failure avoidance or class discrimination may also be use- ful. We suggest the application of measures which evaluate such properties. These measures – Youden's index, likeli- hood, Discriminant power – are used in medical diagnosis. We show that these measures are interrelated, and we apply them to a case study from the field of electronic negotiations. We also list other learning problems which may benefit from the application of the proposed measures. Introduction},
author = {Sokolova, Marina and Japkowicz, Nathalie and Szpakowicz, Stan},
doi = {10.1007/11941439_114},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sokolova, Japkowicz, Szpakowicz - 2006 - Beyond Accuracy, F-Score and ROC A Family of Discriminant Measures for Performance Evaluation.pdf:pdf},
isbn = {978-3-540-49787-5},
issn = {0302-9743},
keywords = {Copyright {\textcopyright}2006, American Association for Artifici},
number = {c},
pages = {1015--1021},
title = {{Beyond Accuracy, F-Score and ROC: A Family of Discriminant Measures for Performance Evaluation}},
url = {http://link.springer.com/10.1007/11941439{\_}114},
year = {2006}
}
@book{Rossum1996,
author = {Watters, Aaron and {Van Rossum}, Guido and Alstrom, James C.},
editor = {Books, M{\&}T},
isbn = {1558514848, 9781558514843},
pages = {477},
title = {{Internet Programming with Python}},
year = {1996}
}
@article{TibaduizaBurgos2013,
abstract = {In previous works, the authors showed advantages and drawbacks of the use of PCA and ICA by separately. In this paper, a comparison of results in the application of these methodologies is presented. Both of them exploit the advantage of using a piezoelectric active system in different phases. An initial baseline model for the undamaged structure is built applying each technique to the data collected by several experiments. The current structure (damaged or not) is subjected to the same experiments and the collected data are projected into the models. In order to determine whether damage exists or not in the structure, the projections into the first and second components using PCA and ICA are depicted graphically. A comparison between these plots is performed analyzing differences and similarities, advantages and drawbacks. To validate the approach, the methodology is applied in two sections of an aircraft wing skeleton powered with several PZTs transducers.},
author = {{Tibaduiza Burgos}, Diego Alexander and {Mujica Delgado}, Luis Eduardo and Anaya, Maribel and {Rodellar Bened{\'{e}}}, Jos{\'{e}} and {G{\"{u}}emes Gordo}, Alfredo},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/PCAvsICA.pdf:pdf},
pages = {1--8},
title = {{Principal component analysis vs. independent component analysis for damage detection}},
year = {2013}
}
@article{VanderMaaten2014,
abstract = {The paper investigates the acceleration of t-SNE—an embedding technique that is com- monly used for the visualization of high-dimensional data in scatter plots—using two tree- based algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE em- beddings in O(N logN). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {van der Maaten, Laurens},
doi = {10.1007/978-1-60761-580-4_8},
eprint = {1307.1662},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/accelerating-tsne-using-tree-based-algorithms.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Barnes-Hut algorithm,dual-tree algorithm,embedding,multidimensional scaling,space-partitioning trees,t-SNE},
pages = {3221--3245},
pmid = {20652508},
title = {{Accelerating t-SNE using Tree-Based Algorithms}},
url = {dl.acm.org/citation.cfm?id=2697068},
volume = {15},
year = {2014}
}
@article{Alferez2015,
author = {Alf{\'{e}}rez, S},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alf{\'{e}}rez - 2015 - Methodology for Automatic Classification of Atypical Lymphoid Cells from Peripheral Blood Cell Images.pdf:pdf;:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Copy of Thesis{\_}Edwin{\_}Alferez{\_}vPrint.pdf:pdf},
number = {February},
pages = {181},
title = {{Methodology for Automatic Classification of Atypical Lymphoid Cells from Peripheral Blood Cell Images}},
year = {2015}
}
@article{Helwig2015,
author = {Helwig, Author Nathaniel E and Helwig, Maintainer Nathaniel E},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/ica.pdf:pdf},
title = {{Package ‘ ica '}},
year = {2015}
}
@article{Lee2012,
author = {Lee, Yuh-jye and Yeh, Yi-ren and Wang, Y},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Anomaly Detection via Online Over-Sampling Principal Component Analysis.pdf:pdf},
issn = {1041-4347},
journal = {Tkde},
number = {Xx},
pages = {1--12},
title = {{Anomaly detection via online over-sampling principal component analysis}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6200273},
volume = {XX},
year = {2012}
}
@article{Osgood2000,
abstract = {This article introduces the use of regression models based on the Poisson distri-bution as a tool for resolving common problems in analyzing aggregate crime rates. When the population size of an aggregate unit is small relative to the offense rate, crime rates must be computed from a small number of offenses. Such data are ill-suited to least-squares analysis. Poisson-based regression models of counts of offenses are preferable because they are built on assumptions about error distributions that are consistent with the nature of event counts. A simple elaboration transforms the Poisson model of offense counts to a model of per capita offense rates. To demonstrate the use and advantages of this method, this article presents analyses of juvenile arrest rates for robbery in 264 nonmetropoli-tan counties in four states. The negative binomial variant of Poisson regression effectively resolved difficulties that arise in ordinary least-squares analyses.},
author = {Osgood, D. Wayne},
doi = {10.1023/a:1007521427059},
file = {:D$\backslash$:/MU{\_}Bioinformatica/Regresion y Metodos/PEC{\_}1/Materiales/Poisson-based-regression-of-aggregated-crime-rates.pdf:pdf},
isbn = {0748-4518},
issn = {0748-4518},
journal = {Journal of Quantitative Criminology},
keywords = {aggregate analysis,crime rates,negative binomial,poisson},
number = {1},
pages = {21--43},
title = {{Poisson-based regression analysis of aggregate crime rates}},
volume = {16},
year = {2000}
}
@article{Granger2011,
author = {Granger, Brian E and Hunter, John D},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/python-an-ecosystem-for-scientific-computing.pdf:pdf},
pages = {13--21},
title = {{Python : An Ecosystem}},
year = {2011}
}
@article{Soulie1987,
author = {Soulie, Fogelman and Gallinari, F. and P. and Lecun, Yann and Thiria, S.},
journal = {Automata networks in computer science, theory and applications},
pages = {133--186},
title = {{Automata networks and artificial intelligence}},
year = {1987}
}
