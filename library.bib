Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{ufldltutorial,
author = {University, Stanford},
title = {{Unsupervised Feature Learning and Deep Learning Tutorial}},
url = {ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/}
}
@article{Benattar2001,
abstract = {Distinguishing leukemic phases of B-cell disorders in peripheral blood smears is well recognized to be difficult in some cases since it depends on subtle and subjective criteria. In order to quantify cytological features and to assess objective descriptions, a morphometric analysis was performed on 83 peripheral blood smears of B-cells disorders (n = 77) and healthy donors (n = 6). Using standardized May-Grunwald Giemsa staining, standardized image acquisition system and well defined microscopic fields, we have analyzed lymphoid cells, measuring morphometric and color parameters. By combining seven relevant morphometric criteria (the nuclear shape, the cellular shape and area, the nucleo-cytoplasmic ratio, the nuclear red/blue ratio, the cytoplasmic green/blue ratio and the proportion of cells with nucleolus), we have established a score that could range from a minimum of -3 (large B-CLL type) to a maximum of +8 (large MCL type): negative scores corresponds to different types of B-CLL (n = 30), including “atypical B-CLL” (n = 6), the score zero correspond to healthy donors (n = 6) used as baseline, the positive score values correspond to +1 for Follicular lymphoma (n = 2), +3 for Splenic Lymphoma with Villous Lymphocytes (n = 12), +4 for Hairy Cell Leukemia (n = 7), +5 for Hairy Cell Leukemia-variant (n = 2), +6 for B-prolymphocytic leukemia (n = 6) and +7 and +8 for most Mantle Cell Lymphoma (n = 18). Testing T-cell disorders samples (n = 10) using the same protocol, the profile is different and cannot be confused with B-cell diseases.Our scoring system indicates that measurement of some common morphologic features in standardized conditions provides objective criteria to characterize those diseases and might be helpful for diagnosis.},
author = {Benattar, Laurence and Flandrin, Georges},
doi = {10.3109/10428190109097674},
journal = {Leukemia {\&} Lymphoma},
number = {1-2},
pages = {29--40},
pmid = {11699219},
title = {{Morphometric and Colorimetric Analysis of Peripheral Blood Smears Lymphocytes in B-Cell Disorders: Proposal for a Scoring System}},
url = {http://dx.doi.org/10.3109/10428190109097674},
volume = {42},
year = {2001}
}
@article{Puigvi2017,
author = {Puigv{\'{i}}, L and Merino, A and Alf{\'{e}}rez, S and Acevedo, A and Rodellar, J},
doi = {10.1136/jclinpath-2017-204389},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/NewQuantitative{\_}2017.pdf:pdf},
issn = {14724146},
journal = {J Clin Pathol},
pages = {Published Online First: 13 June 2017},
title = {{New quantitative features for the morphological differentiation of abnormal lymphoid cell images from peripheral blood}},
year = {2017}
}
@article{Alferez2015,
author = {Alf{\'{e}}rez, S},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alf{\'{e}}rez - 2015 - Methodology for Automatic Classification of Atypical Lymphoid Cells from Peripheral Blood Cell Images.pdf:pdf;:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Copy of Thesis{\_}Edwin{\_}Alferez{\_}vPrint.pdf:pdf},
number = {February},
pages = {181},
title = {{Methodology for Automatic Classification of Atypical Lymphoid Cells from Peripheral Blood Cell Images}},
year = {2015}
}
@article{TibaduizaBurgos2013,
abstract = {In previous works, the authors showed advantages and drawbacks of the use of PCA and ICA by separately. In this paper, a comparison of results in the application of these methodologies is presented. Both of them exploit the advantage of using a piezoelectric active system in different phases. An initial baseline model for the undamaged structure is built applying each technique to the data collected by several experiments. The current structure (damaged or not) is subjected to the same experiments and the collected data are projected into the models. In order to determine whether damage exists or not in the structure, the projections into the first and second components using PCA and ICA are depicted graphically. A comparison between these plots is performed analyzing differences and similarities, advantages and drawbacks. To validate the approach, the methodology is applied in two sections of an aircraft wing skeleton powered with several PZTs transducers.},
author = {{Tibaduiza Burgos}, Diego Alexander and {Mujica Delgado}, Luis Eduardo and Anaya, Maribel and {Rodellar Bened{\'{e}}}, Jos{\'{e}} and {G{\"{u}}emes Gordo}, Alfredo},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/PCAvsICA.pdf:pdf},
pages = {1--8},
title = {{Principal component analysis vs. independent component analysis for damage detection}},
year = {2013}
}
@book{StanleyAMulaik2009,
abstract = {Providing a practical, thorough understanding of how factor analysis works, Foundations of Factor Analysis, Second Edition discusses the assumptions underlying the equations and procedures of this method. It also explains the options in commercial computer programs for performing factor analysis and structural equation modeling. This long-awaited edition takes into account the various developments that have occurred since the publication of the original edition.},
author = {{Stanley A Mulaik}},
edition = {2nd, Illus},
editor = {{CRC Press}, 2009},
isbn = {1420099817, 9781420099812},
pages = {548},
title = {{Foundations of Factor Analysis, Second Edition}},
year = {2009}
}
@article{Soulie1987,
author = {Soulie, Fogelman and Gallinari, F. and P. and Lecun, Yann and Thiria, S.},
journal = {Automata networks in computer science, theory and applications},
pages = {133--186},
title = {{Automata networks and artificial intelligence}},
year = {1987}
}
@article{Bro2014,
abstract = {{\textless}p{\textgreater}Principal component analysis is one of the most important and powerful methods in chemometrics as well as in a wealth of other areas.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bro, Rasmus and Smilde, Age K.},
doi = {10.1039/C3AY41907J},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bro, Smilde - 2014 - Principal component analysis.pdf:pdf},
isbn = {1939-0068},
issn = {1759-9660},
journal = {Anal. Methods},
number = {9},
pages = {2812--2831},
pmid = {20931840},
title = {{Principal component analysis}},
url = {http://xlink.rsc.org/?DOI=C3AY41907J},
volume = {6},
year = {2014}
}
@article{Garrett-Mayer2006,
abstract = {Slide set},
author = {Garrett-Mayer, Elizabeth},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garrett-Mayer - 2006 - Statistics in Psychosocial Lecture 8 Factor Analysis.pdf:pdf},
journal = {The Johns Hopkins University},
pages = {1--56},
title = {{Statistics in Psychosocial Lecture 8: Factor Analysis}},
year = {2006}
}
@book{HarryHHarman1976,
abstract = {This thoroughly revised third edition of Harry H. Harman's authoritative text incorporates the many new advances made in computer science and technology over the last ten years. The author gives full coverage to both theoretical and applied aspects of factor analysis from its foundations through the most advanced techniques. This highly readable text will be welcomed by researchers and students working in psychology, statistics, economics, and related disciplines.},
author = {{Harry H. Harman}},
edition = {Illustrate},
editor = {{University of Chicago Press}, 1976},
isbn = {0226316521, 9780226316529},
pages = {269},
title = {{Modern Factor Analysis}},
year = {1976}
}
@article{Burges2010,
abstract = {We give a tutorial overview of several foundational methods for dimension reduction. We divide the methods into projective methods and methods that model the manifold on which the data lies. For projective methods, we review projection pursuit, principal component analysis (PCA), kernel PCA, probabilistic PCA, canonical correlation analysis (CCA), kernel CCA, Fisher discriminant analysis, oriented PCA, and several techniques for sufficient dimension reduction. For the manifold methods, we review multidimensional scaling (MDS), landmark MDS, Isomap, locally linear embedding, Laplacian eigenmaps, and spectral clustering. Although the review focuses on foundations, we also provide pointers to some more modern techniques. We also describe the correlation dimension as one method for estimating the intrinsic dimension, and we point out that the notion of dimension can be a scale-dependent quantity. The Nystr{\"{o}}m method, which links several of the manifold algorithms, is also reviewed. We use a publicly available dataset to illustrate some of the methods. The goal is to provide a self-contained overview of key concepts underlying many of these algorithms, and to give pointers for further reading.},
author = {Burges, Cjc},
doi = {10.1561/2200000002},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/dimension{\_}reduction{\_}a{\_}guided{\_}tour.pdf:pdf},
isbn = {9781601983787},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
number = {4},
pages = {275--365},
title = {{Dimension reduction: A guided tour}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL{\&}doi=2200000002{\%}5Cnhttp://research.microsoft.com/pubs/150728/FnT{\_}dimensionReduction.pdf},
volume = {2},
year = {2010}
}
@article{Farrelly,
author = {Farrelly, Colleen M},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/reviewofmethodsfordimensionreduction.pdf:pdf},
title = {{Review of Methods for Dimension Reduction Reasons to Reduce}}
}
@book{Tryfos1998,
abstract = {Focusing on the principal methods for analysis and forecasting, the text is designed to fit two types of course design, the traditional approach for a technical course or the frequently used approach in business courses. Examples, exercises, problems and small and large cases are provided to fit into either or both approaches. An instructor's manual is available which includes solutions, author's notes of each case and possible alternatives, and the programs used by the author.},
author = {Gorsuch, Richard L.},
edition = {English},
editor = {{Wiley; 1 edition (January 23}, 1998)},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Example - 2001 - Chapter 14 Factor analysis.pdf:pdf},
isbn = {978-0471123842},
pages = {592},
title = {{Factor analysis}},
year = {1998}
}
@article{Tipping1999,
abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss the advantages conveyed by the definition of a probability density function for PCA.},
author = {Tipping, Michael E. and Bishop, Christopher M.},
doi = {10.1111/1467-9868.00196},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tipping, Bishop - 1999 - Probabilistic Principal Component Analysis.pdf:pdf},
isbn = {9781424444427},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
number = {3},
pages = {611--622},
pmid = {21092268},
title = {{Probabilistic Principal Component Analysis}},
url = {http://doi.wiley.com/10.1111/1467-9868.00196},
volume = {61},
year = {1999}
}
@article{Smith2002,
abstract = {This tutorial is designed to give the reader an understanding of Principal Components Analysis (PCA). PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. Before getting to a description of PCA, this tutorial first introduces mathematical concepts that will be used in PCA. It covers standard deviation, covariance, eigenvec- tors and eigenvalues. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar. There are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. If further information is required, the mathematics textbook Elementary Linear Algebra 5e by Howard Anton, Publisher JohnWiley {\&} Sons Inc, ISBN 0-471-85223-6 is a good source of information regarding the mathematical back- ground. 1},
archivePrefix = {arXiv},
arxivId = {1511.06448},
author = {Smith, Lindsay I},
doi = {10.1080/03610928808829796},
eprint = {1511.06448},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith - 2002 - A tutorial on Principal Components Analysis Introduction.pdf:pdf},
isbn = {0471852236},
issn = {03610926},
journal = {Statistics},
keywords = {analysis,utorial on principal components},
pages = {52},
pmid = {16765218},
title = {{A tutorial on Principal Components Analysis Introduction}},
url = {http://www.mendeley.com/research/computational-genome-analysis-an-introduction-statistics-for-biology-and-health/},
volume = {51},
year = {2002}
}
@article{Chen2017,
abstract = {Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
doi = {10.1016/j.eswa.2016.10.065},
eprint = {1404.7828},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/deep{\_}learning{\_}in{\_}neural{\_}networks{\_}an{\_}overview.pdf:pdf},
isbn = {0925-2312},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deep neural network,Natural language processing,Sentiment analysis},
pages = {221--230},
pmid = {19932002},
title = {{Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN}},
volume = {72},
year = {2017}
}
@article{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jolliffe, I T},
doi = {10.2307/1270093},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jolliffe - 2002 - Principal Component Analysis, Second Edition.pdf:pdf},
isbn = {0387954422},
issn = {00401706},
journal = {Encyclopedia of Statistics in Behavioral Science},
number = {3},
pages = {487},
pmid = {21435900},
title = {{Principal Component Analysis, Second Edition}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/0470013192.bsa501/full},
volume = {30},
year = {2002}
}
@article{Hyvarinen2000,
abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of nongaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Hyv{\"{a}}rinen, Aapo and Oja, Erkki},
doi = {10.1016/S0893-6080(00)00026-5},
eprint = {1504.05070},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/ICA{\_}Hyvarinen.pdf:pdf},
isbn = {3589451327},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {Independent component analysis,blind signal separation,factor analysis,projection pursuit,representation,source separation},
number = {45},
pages = {411--430},
pmid = {10946390},
title = {{Independent Component Analysis: Algorithms and Applications}},
volume = {13},
year = {2000}
}
@article{Stone2005,
abstract = {Given a set ofM signal mixtures (x1,x2, . . . ,xM) (e.g. microphone outputs), each of which is a different mixture of a set of M statistically independent source signals (s1, s2, . . . , sM) (e.g. voices), independent component analysis (ICA) recovers the source signals (voices) from the signal mixtures. ICA is based on the assumptions that source signals are statistically independent and that they have non-Gaussian distributions. Different physical processes usually generate statistically independent and non-Gaussian signals, so that, in the process of extracting such signals from a set of signal mixtures, ICA effectively recovers the underlying physical causes for a given set of measured signal mixtures.},
author = {Stone, James V.},
doi = {10.1002/0470013192.bsa297},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/ica{\_}encyc{\_}jvs4everrit2005.pdf:pdf},
isbn = {9780470860809},
journal = {Encyclopedia of Statistics in Behavioral Science},
keywords = {Independent Component Analysis},
number = {Novembre},
pages = {1--11},
title = {{Independent Component Analysis}},
url = {http://doi.wiley.com/10.1002/0470013192.bsa297},
volume = {2},
year = {2005}
}
@article{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/3048-greedy-layer-wise-training-of-deep-networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {153},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
volume = {19},
year = {2007}
}
@article{Ballard1987,
abstract = {In the development of large-scale knowledge networks , much recent progress has been inspired by connections to neurobiology. An important component of any" neural " network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must ... $\backslash$n},
author = {Ballard, Dana H.},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/modular{\_}learning{\_}in{\_}neural{\_}networks.pdf:pdf},
isbn = {preprint},
journal = {Aaai},
pages = {279--284},
title = {{Modular Learning in Neural Networks}},
year = {1987}
}
@article{VanDerMaaten2009,
abstract = {In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear tech- niques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identi- fying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.},
author = {{Van Der Maaten}, Laurens and Postma, Eric and {Van Den Herik}, Jaap},
doi = {10.1080/13506280444000102},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/dimensionality{\_}reduction{\_}a{\_}comparative{\_}review.pdf:pdf},
issn = {0169328X},
journal = {October},
pages = {1--35},
pmid = {7877450},
title = {{Dimensionality Reduction : A Comparative Review}},
url = {http://www.uvt.nl/ticc},
year = {2009}
}
@article{Ng2011,
abstract = {Beschreibt Neuronale Netze, Backpropagation und (Sparse) Auto-Encoders},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Ng, Andrew},
doi = {10.1371/journal.pone.0006098},
eprint = {arXiv:1506.03733v1},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Sparse{\_}autoencoder.pdf:pdf},
isbn = {1595937935},
issn = {19326203},
journal = {CS294A Lecture notes},
pages = {1--19},
pmid = {19568420},
title = {{Sparse autoencoder}},
url = {http://www.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf},
year = {2011}
}
@article{Pet2010,
author = {Petˇ, M},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/WDS10{\_}113{\_}i2{\_}Petricek.pdf:pdf},
isbn = {9788073781392},
pages = {82--87},
title = {{Components in Data Analysis}},
volume = {1},
year = {2010}
}
@article{Bengio2013,
abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).},
archivePrefix = {arXiv},
arxivId = {1305.6663},
author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
eprint = {1305.6663},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/generalized-denoising-auto-encoders-as-generative-models.pdf:pdf},
isbn = {10495258},
issn = {10495258},
pages = {1--9},
title = {{Generalized Denoising Auto-Encoders as Generative Models}},
url = {http://arxiv.org/abs/1305.6663},
year = {2013}
}
@article{Rifai2011,
abstract = {Although the structure and composition of plant communities is known to influence the functioning of ecosystems, there is as yet no agreement as to how these should be described from a functional perspective. We tested the biomass ratio hypothesis, which postulates that ecosystem properties should depend on species traits and on species contribution to the total biomass of the community, in a successional sere following vineyard abandonment in the Mediterranean region of France. Ecosystem-specific net primary productivity, litter decomposition rate, and total soil carbon and nitrogen varied significantly with field age, and correlated with community-aggregated (i.e., weighed according to the relative abundance of species) functional leaf traits. The three easily measurable traits tested, specific leaf area, leaf dry matter content, and nitrogen concentration, provide a simple means to scale up from organ to ecosystem functioning in complex plant communities. We propose that they be called {\&}8220;functional markers,{\&}8221; and be used to assess the impacts of community changes on ecosystem properties induced, in particular, by global change drivers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rifai, Salah and Muller, Xavier},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/contractive{\_}autoencoders{\_}feature{\_}extraction.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {1467-9280},
journal = {Icml},
number = {1},
pages = {833--840},
pmid = {25052830},
title = {{Contractive Auto-Encoders : Explicit Invariance During Feature Extraction}},
url = {http://www.icml-2011.org/papers/455{\_}icmlpaper.pdf},
volume = {85},
year = {2011}
}
@article{Bigorra2017,
author = {Bigorra, Laura and Merino, Anna and Alf{\'{e}}rez, Santiago and Rodellar, Jos{\'{e}}},
doi = {10.1002/jcla.22024},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/FeatureAnalysis{\_}2016.pdf:pdf},
issn = {10982825},
journal = {Journal of Clinical Laboratory Analysis},
keywords = {cytology,hematology,image processing,leukemia,pathology},
number = {2},
pages = {1--9},
title = {{Feature Analysis and Automatic Identification of Leukemic Lineage Blast Cells and Reactive Lymphoid Cells from Peripheral Blood Cell Images}},
volume = {31},
year = {2017}
}
@article{Fodor2002,
abstract = {this paper, we assume that we have n observations, each being a realization of the p- dimensional random variable x = (x 1 , . . . , x p )    with mean E(x) =  = ( 1 , . . . ,  p )    and covariance matrix E{\{}(x    )(x      = {\#} pp . We denote such an observation matrix by X =    i,j : 1      p, 1      n{\}}. If  i and {\#} i =    {\#} (i,i) denote the mean and the standard deviation of the ith random variable, respectively, then we will often standardize the observations x i,j by (x i,j      i )/  {\#} i , where   i =  x i = 1/n    j=1 x i,j , and  {\#} i = 1/n    j=1 (x i,j     x i )},
author = {Fodor, Imola},
doi = {10.1.1.8.5098},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/a{\_}survey{\_}of{\_}dimension{\_}reduction{\_}techniques.pdf:pdf},
title = {{A Survey of Dimension Reduction Techniques}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.5098},
year = {2002}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Hyvarinen97.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Independent Component Analysis by Minimization of Mutual Information}},
volume = {53},
year = {2013}
}
@article{Guyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/an{\_}introduction{\_}to{\_}variable{\_}selection.pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Hyvarinen2001,
abstract = {A comprehensive introduction to ICA for students and practitioners Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more. Independent Component Analysis is divided into four sections that cover: General mathematical concepts utilized in the book The basic ICA model and its solution Various extensions of the basic ICA model Real-world applications for ICA models Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.},
author = {Hyv{\"{a}}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
doi = {10.1002/0471221317},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/bookfinal{\_}ICA.pdf:pdf},
isbn = {047140540X},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
number = {1},
pages = {135--144},
pmid = {20421937},
title = {{Independent Component Analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520306000509{\%}5Cnhttp://doi.wiley.com/10.1002/0471221317},
volume = {21},
year = {2001}
}
@article{Vincent2008,
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6550v4},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1145/1390156.1390294},
eprint = {arXiv:1412.6550v4},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/extracting{\_}and{\_}compsoing{\_}robust{\_}features{\_}autoencoders.pdf:pdf},
isbn = {9781605582054},
issn = {1605582050},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
number = {July},
pages = {1096--1103},
pmid = {15540460},
title = {{Extracting and composing robust features with denoising autoencoders}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390294},
year = {2008}
}
