Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Tsne2016,
author = {Package, Type},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/tsne.pdf:pdf},
pages = {2--5},
title = {{Package ‘ tsne '}},
year = {2016}
}
@article{Vincent2008,
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6550v4},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1145/1390156.1390294},
eprint = {arXiv:1412.6550v4},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/extracting{\_}and{\_}compsoing{\_}robust{\_}features{\_}autoencoders.pdf:pdf},
isbn = {9781605582054},
issn = {1605582050},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
number = {July},
pages = {1096--1103},
pmid = {15540460},
title = {{Extracting and composing robust features with denoising autoencoders}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390294},
year = {2008}
}
@article{VanderMaaten2014,
abstract = {The paper investigates the acceleration of t-SNE—an embedding technique that is com- monly used for the visualization of high-dimensional data in scatter plots—using two tree- based algorithms. In particular, the paper develops variants of the Barnes-Hut algorithm and of the dual-tree algorithm that approximate the gradient used for learning t-SNE em- beddings in O(N logN). Our experiments show that the resulting algorithms substantially accelerate t-SNE, and that they make it possible to learn embeddings of data sets with millions of objects. Somewhat counterintuitively, the Barnes-Hut variant of t-SNE appears to outperform the dual-tree variant.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {van der Maaten, Laurens},
doi = {10.1007/978-1-60761-580-4_8},
eprint = {1307.1662},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/accelerating-tsne-using-tree-based-algorithms.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Barnes-Hut algorithm,dual-tree algorithm,embedding,multidimensional scaling,space-partitioning trees,t-SNE},
pages = {3221--3245},
pmid = {20652508},
title = {{Accelerating t-SNE using Tree-Based Algorithms}},
url = {dl.acm.org/citation.cfm?id=2697068},
volume = {15},
year = {2014}
}
@article{Jolliffe2002,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jolliffe, I T},
doi = {10.2307/1270093},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jolliffe - 2002 - Principal Component Analysis, Second Edition.pdf:pdf},
isbn = {0387954422},
issn = {00401706},
journal = {Encyclopedia of Statistics in Behavioral Science},
number = {3},
pages = {487},
pmid = {21435900},
title = {{Principal Component Analysis, Second Edition}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/0470013192.bsa501/full},
volume = {30},
year = {2002}
}
@misc{ufldltutorial,
author = {University, Stanford},
title = {{Unsupervised Feature Learning and Deep Learning Tutorial}},
url = {ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/scikit-learn{\_}machine-learning-in-python.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Tipping1999,
abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss the advantages conveyed by the definition of a probability density function for PCA.},
author = {Tipping, Michael E. and Bishop, Christopher M.},
doi = {10.1111/1467-9868.00196},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tipping, Bishop - 1999 - Probabilistic Principal Component Analysis.pdf:pdf},
isbn = {9781424444427},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
number = {3},
pages = {611--622},
pmid = {21092268},
title = {{Probabilistic Principal Component Analysis}},
url = {http://doi.wiley.com/10.1111/1467-9868.00196},
volume = {61},
year = {1999}
}
@article{Smith2002,
abstract = {This tutorial is designed to give the reader an understanding of Principal Components Analysis (PCA). PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. Before getting to a description of PCA, this tutorial first introduces mathematical concepts that will be used in PCA. It covers standard deviation, covariance, eigenvec- tors and eigenvalues. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar. There are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. If further information is required, the mathematics textbook Elementary Linear Algebra 5e by Howard Anton, Publisher JohnWiley {\&} Sons Inc, ISBN 0-471-85223-6 is a good source of information regarding the mathematical back- ground. 1},
archivePrefix = {arXiv},
arxivId = {1511.06448},
author = {Smith, Lindsay I},
doi = {10.1080/03610928808829796},
eprint = {1511.06448},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith - 2002 - A tutorial on Principal Components Analysis Introduction.pdf:pdf},
isbn = {0471852236},
issn = {03610926},
journal = {Statistics},
keywords = {analysis,utorial on principal components},
pages = {52},
pmid = {16765218},
title = {{A tutorial on Principal Components Analysis Introduction}},
url = {http://www.mendeley.com/research/computational-genome-analysis-an-introduction-statistics-for-biology-and-health/},
volume = {51},
year = {2002}
}
@article{Alferez2015,
author = {Alf{\'{e}}rez, S},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alf{\'{e}}rez - 2015 - Methodology for Automatic Classification of Atypical Lymphoid Cells from Peripheral Blood Cell Images.pdf:pdf;:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Copy of Thesis{\_}Edwin{\_}Alferez{\_}vPrint.pdf:pdf},
number = {February},
pages = {181},
title = {{Methodology for Automatic Classification of Atypical Lymphoid Cells from Peripheral Blood Cell Images}},
year = {2015}
}
@article{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/3048-greedy-layer-wise-training-of-deep-networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {153},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
volume = {19},
year = {2007}
}
@article{Bengio2013,
abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).},
archivePrefix = {arXiv},
arxivId = {1305.6663},
author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
eprint = {1305.6663},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/generalized-denoising-auto-encoders-as-generative-models.pdf:pdf},
isbn = {10495258},
issn = {10495258},
pages = {1--9},
title = {{Generalized Denoising Auto-Encoders as Generative Models}},
url = {http://arxiv.org/abs/1305.6663},
year = {2013}
}
@article{Fodor2002,
abstract = {this paper, we assume that we have n observations, each being a realization of the p- dimensional random variable x = (x 1 , . . . , x p )    with mean E(x) =  = ( 1 , . . . ,  p )    and covariance matrix E{\{}(x    )(x      = {\#} pp . We denote such an observation matrix by X =    i,j : 1      p, 1      n{\}}. If  i and {\#} i =    {\#} (i,i) denote the mean and the standard deviation of the ith random variable, respectively, then we will often standardize the observations x i,j by (x i,j      i )/  {\#} i , where   i =  x i = 1/n    j=1 x i,j , and  {\#} i = 1/n    j=1 (x i,j     x i )},
author = {Fodor, Imola},
doi = {10.1.1.8.5098},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/a{\_}survey{\_}of{\_}dimension{\_}reduction{\_}techniques.pdf:pdf},
title = {{A Survey of Dimension Reduction Techniques}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.5098},
year = {2002}
}
@article{TibaduizaBurgos2013,
abstract = {In previous works, the authors showed advantages and drawbacks of the use of PCA and ICA by separately. In this paper, a comparison of results in the application of these methodologies is presented. Both of them exploit the advantage of using a piezoelectric active system in different phases. An initial baseline model for the undamaged structure is built applying each technique to the data collected by several experiments. The current structure (damaged or not) is subjected to the same experiments and the collected data are projected into the models. In order to determine whether damage exists or not in the structure, the projections into the first and second components using PCA and ICA are depicted graphically. A comparison between these plots is performed analyzing differences and similarities, advantages and drawbacks. To validate the approach, the methodology is applied in two sections of an aircraft wing skeleton powered with several PZTs transducers.},
author = {{Tibaduiza Burgos}, Diego Alexander and {Mujica Delgado}, Luis Eduardo and Anaya, Maribel and {Rodellar Bened{\'{e}}}, Jos{\'{e}} and {G{\"{u}}emes Gordo}, Alfredo},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/PCAvsICA.pdf:pdf},
pages = {1--8},
title = {{Principal component analysis vs. independent component analysis for damage detection}},
year = {2013}
}
@article{Farrelly,
author = {Farrelly, Colleen M},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/reviewofmethodsfordimensionreduction.pdf:pdf},
title = {{Review of Methods for Dimension Reduction Reasons to Reduce}}
}
@article{Hyvarinen2001,
abstract = {A comprehensive introduction to ICA for students and practitioners Independent Component Analysis (ICA) is one of the most exciting new topics in fields such as neural networks, advanced statistics, and signal processing. This is the first book to provide a comprehensive introduction to this new technique complete with the fundamental mathematical background needed to understand and utilize it. It offers a general overview of the basics of ICA, important solutions and algorithms, and in-depth coverage of new applications in image processing, telecommunications, audio signal processing, and more. Independent Component Analysis is divided into four sections that cover: General mathematical concepts utilized in the book The basic ICA model and its solution Various extensions of the basic ICA model Real-world applications for ICA models Authors Hyvarinen, Karhunen, and Oja are well known for their contributions to the development of ICA and here cover all the relevant theory, new algorithms, and applications in various fields. Researchers, students, and practitioners from a variety of disciplines will find this accessible volume both helpful and informative.},
author = {Hyv{\"{a}}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
doi = {10.1002/0471221317},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/bookfinal{\_}ICA.pdf:pdf},
isbn = {047140540X},
issn = {10635203},
journal = {Applied and Computational Harmonic Analysis},
number = {1},
pages = {135--144},
pmid = {20421937},
title = {{Independent Component Analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1063520306000509{\%}5Cnhttp://doi.wiley.com/10.1002/0471221317},
volume = {21},
year = {2001}
}
@article{Bro2014,
abstract = {{\textless}p{\textgreater}Principal component analysis is one of the most important and powerful methods in chemometrics as well as in a wealth of other areas.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bro, Rasmus and Smilde, Age K.},
doi = {10.1039/C3AY41907J},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bro, Smilde - 2014 - Principal component analysis.pdf:pdf},
isbn = {1939-0068},
issn = {1759-9660},
journal = {Anal. Methods},
number = {9},
pages = {2812--2831},
pmid = {20931840},
title = {{Principal component analysis}},
url = {http://xlink.rsc.org/?DOI=C3AY41907J},
volume = {6},
year = {2014}
}
@article{Garrett-Mayer2006,
abstract = {Slide set},
author = {Garrett-Mayer, Elizabeth},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garrett-Mayer - 2006 - Statistics in Psychosocial Lecture 8 Factor Analysis.pdf:pdf},
journal = {The Johns Hopkins University},
pages = {1--56},
title = {{Statistics in Psychosocial Lecture 8: Factor Analysis}},
year = {2006}
}
@article{Bigorra2017,
author = {Bigorra, Laura and Merino, Anna and Alf{\'{e}}rez, Santiago and Rodellar, Jos{\'{e}}},
doi = {10.1002/jcla.22024},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/FeatureAnalysis{\_}2016.pdf:pdf},
issn = {10982825},
journal = {Journal of Clinical Laboratory Analysis},
keywords = {cytology,hematology,image processing,leukemia,pathology},
number = {2},
pages = {1--9},
title = {{Feature Analysis and Automatic Identification of Leukemic Lineage Blast Cells and Reactive Lymphoid Cells from Peripheral Blood Cell Images}},
volume = {31},
year = {2017}
}
@article{Puigvi2017,
author = {Puigv{\'{i}}, L and Merino, A and Alf{\'{e}}rez, S and Acevedo, A and Rodellar, J},
doi = {10.1136/jclinpath-2017-204389},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/NewQuantitative{\_}2017.pdf:pdf},
issn = {14724146},
journal = {J Clin Pathol},
pages = {Published Online First: 13 June 2017},
title = {{New quantitative features for the morphological differentiation of abnormal lymphoid cell images from peripheral blood}},
year = {2017}
}
@article{Helwig2015,
author = {Helwig, Author Nathaniel E and Helwig, Maintainer Nathaniel E},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/ica.pdf:pdf},
title = {{Package ‘ ica '}},
year = {2015}
}
@article{Rifai2011,
abstract = {Although the structure and composition of plant communities is known to influence the functioning of ecosystems, there is as yet no agreement as to how these should be described from a functional perspective. We tested the biomass ratio hypothesis, which postulates that ecosystem properties should depend on species traits and on species contribution to the total biomass of the community, in a successional sere following vineyard abandonment in the Mediterranean region of France. Ecosystem-specific net primary productivity, litter decomposition rate, and total soil carbon and nitrogen varied significantly with field age, and correlated with community-aggregated (i.e., weighed according to the relative abundance of species) functional leaf traits. The three easily measurable traits tested, specific leaf area, leaf dry matter content, and nitrogen concentration, provide a simple means to scale up from organ to ecosystem functioning in complex plant communities. We propose that they be called {\&}8220;functional markers,{\&}8221; and be used to assess the impacts of community changes on ecosystem properties induced, in particular, by global change drivers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rifai, Salah and Muller, Xavier},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/contractive{\_}autoencoders{\_}feature{\_}extraction.pdf:pdf},
isbn = {978-1-4503-0619-5},
issn = {1467-9280},
journal = {Icml},
number = {1},
pages = {833--840},
pmid = {25052830},
title = {{Contractive Auto-Encoders : Explicit Invariance During Feature Extraction}},
url = {http://www.icml-2011.org/papers/455{\_}icmlpaper.pdf},
volume = {85},
year = {2011}
}
@article{Naugler2014,
abstract = {Peripheral blood smear image examination is a part of the routine work of every laboratory. The manual examination of these images is tedious, time-consuming and suffers from interobserver variation. This has motivated researchers to develop different algorithms and methods to automate peripheral blood smear image analysis. Image analysis itself consists of a sequence of steps consisting of image segmentation, features extraction and selection and pattern classification. The image segmentation step addresses the problem of extraction of the object or region of interest from the complicated peripheral blood smear image. Support vector machine (SVM) and artificial neural networks (ANNs) are two common approaches to image segmentation. Features extraction and selection aims to derive descriptive characteristics of the extracted object, which are similar within the same object class and different between different objects. This will facilitate the last step of the image analysis process: pattern classification. The goal of pattern classification is to assign a class to the selected features from a group of known classes. There are two types of classifier learning algorithms: supervised and unsupervised. Supervised learning algorithms predict the class of the object under test using training data of known classes. The training data have a predefined label for every class and the learning algorithm can utilize this data to predict the class of a test object. Unsupervised learning algorithms use unlabeled training data and divide them into groups using similarity measurements. Unsupervised learning algorithms predict the group to which a new test object belong to, based on the training data without giving an explicit class to that object. ANN, SVM, decision tree and K-nearest neighbor are possible approaches to classification algorithms. Increased discrimination may be obtained by combining several classifiers together.},
author = {Naugler, Christopher and Mohammed, EmadA and Mohamed, MostafaM. A. and Far, BehrouzH},
doi = {10.4103/2153-3539.129442},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/naugler2014.pdf:pdf},
issn = {2153-3539},
journal = {Journal of Pathology Informatics},
keywords = {access this article online,feature extraction,feature selection,microscopic image analysis,peripheral blood smear,segmentation},
number = {1},
pages = {9},
pmid = {24843821},
title = {{Peripheral blood smear image analysis: A comprehensive review}},
url = {http://www.jpathinformatics.org/text.asp?2014/5/1/9/129442},
volume = {5},
year = {2014}
}
@book{StanleyAMulaik2009,
abstract = {Providing a practical, thorough understanding of how factor analysis works, Foundations of Factor Analysis, Second Edition discusses the assumptions underlying the equations and procedures of this method. It also explains the options in commercial computer programs for performing factor analysis and structural equation modeling. This long-awaited edition takes into account the various developments that have occurred since the publication of the original edition.},
author = {{Stanley A Mulaik}},
edition = {2nd, Illus},
editor = {{CRC Press}, 2009},
isbn = {1420099817, 9781420099812},
pages = {548},
title = {{Foundations of Factor Analysis, Second Edition}},
year = {2009}
}
@article{Bunte2012,
abstract = {We present a systematic approach to the mathematical treatment of the t-distributed stochastic neighbor embedding (t-SNE) and the stochastic neighbor embedding (SNE) method. This allows an easy adaptation of the methods or exchange of their respective modules. In particular, the divergence which measures the difference between probability distributions in the original and the embedding space can be treated independently from other components like, e.g. the similarity of data points or the data distribution. We focus on the extension for different divergences and propose a general framework based on the consideration of Fr{\'{e}}chet-derivatives. This way the general approach can be adapted to the user specific needs. {\textcopyright} 2012 Elsevier B.V.},
author = {Bunte, Kerstin and Haase, Sven and Biehl, Michael and Villmann, Thomas},
doi = {10.1016/j.neucom.2012.02.034},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/stochastic-neighbor-embedding-for-dimension-reduction.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Dimension reduction,Divergence optimization,Nonlinear embedding,Stochastic neighbor embedding,Visualization},
pages = {23--45},
publisher = {Elsevier},
title = {{Stochastic neighbor embedding (SNE) for dimension reduction and visualization using arbitrary divergences}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.02.034},
volume = {90},
year = {2012}
}
@article{Ng2011,
abstract = {Beschreibt Neuronale Netze, Backpropagation und (Sparse) Auto-Encoders},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.03733v1},
author = {Ng, Andrew},
doi = {10.1371/journal.pone.0006098},
eprint = {arXiv:1506.03733v1},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Sparse{\_}autoencoder.pdf:pdf},
isbn = {1595937935},
issn = {19326203},
journal = {CS294A Lecture notes},
pages = {1--19},
pmid = {19568420},
title = {{Sparse autoencoder}},
url = {http://www.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf},
year = {2011}
}
@article{Chen2017,
abstract = {Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
doi = {10.1016/j.eswa.2016.10.065},
eprint = {1404.7828},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/deep{\_}learning{\_}in{\_}neural{\_}networks{\_}an{\_}overview.pdf:pdf},
isbn = {0925-2312},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deep neural network,Natural language processing,Sentiment analysis},
pages = {221--230},
pmid = {19932002},
title = {{Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN}},
volume = {72},
year = {2017}
}
@article{Ballard1987,
abstract = {In the development of large-scale knowledge networks , much recent progress has been inspired by connections to neurobiology. An important component of any" neural " network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must ... $\backslash$n},
author = {Ballard, Dana H.},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/modular{\_}learning{\_}in{\_}neural{\_}networks.pdf:pdf},
isbn = {preprint},
journal = {Aaai},
pages = {279--284},
title = {{Modular Learning in Neural Networks}},
year = {1987}
}
@unpublished{RcppDL2015,
author = {Package, Type and Kou, Author Qiang and Sugomori, Yusuke},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/RcppDL.pdf:pdf},
title = {{Package ‘ RcppDL '}},
year = {2015}
}
@misc{Scipy2001,
author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu},
title = {{SciPy: Open source scientific tools for Python}},
url = {http://www.scipy.org/},
urldate = {2017-10-24},
year = {2001}
}
@article{Benattar2001,
abstract = {Distinguishing leukemic phases of B-cell disorders in peripheral blood smears is well recognized to be difficult in some cases since it depends on subtle and subjective criteria. In order to quantify cytological features and to assess objective descriptions, a morphometric analysis was performed on 83 peripheral blood smears of B-cells disorders (n = 77) and healthy donors (n = 6). Using standardized May-Grunwald Giemsa staining, standardized image acquisition system and well defined microscopic fields, we have analyzed lymphoid cells, measuring morphometric and color parameters. By combining seven relevant morphometric criteria (the nuclear shape, the cellular shape and area, the nucleo-cytoplasmic ratio, the nuclear red/blue ratio, the cytoplasmic green/blue ratio and the proportion of cells with nucleolus), we have established a score that could range from a minimum of -3 (large B-CLL type) to a maximum of +8 (large MCL type): negative scores corresponds to different types of B-CLL (n = 30), including “atypical B-CLL” (n = 6), the score zero correspond to healthy donors (n = 6) used as baseline, the positive score values correspond to +1 for Follicular lymphoma (n = 2), +3 for Splenic Lymphoma with Villous Lymphocytes (n = 12), +4 for Hairy Cell Leukemia (n = 7), +5 for Hairy Cell Leukemia-variant (n = 2), +6 for B-prolymphocytic leukemia (n = 6) and +7 and +8 for most Mantle Cell Lymphoma (n = 18). Testing T-cell disorders samples (n = 10) using the same protocol, the profile is different and cannot be confused with B-cell diseases.Our scoring system indicates that measurement of some common morphologic features in standardized conditions provides objective criteria to characterize those diseases and might be helpful for diagnosis.},
author = {Benattar, Laurence and Flandrin, Georges},
doi = {10.3109/10428190109097674},
journal = {Leukemia {\&} Lymphoma},
number = {1-2},
pages = {29--40},
pmid = {11699219},
title = {{Morphometric and Colorimetric Analysis of Peripheral Blood Smears Lymphocytes in B-Cell Disorders: Proposal for a Scoring System}},
url = {http://dx.doi.org/10.3109/10428190109097674},
volume = {42},
year = {2001}
}
@article{Halko2009,
abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, speed, and robustness. These claims are supported by extensive numerical experiments and a detailed error analysis.},
archivePrefix = {arXiv},
arxivId = {0909.4061},
author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
doi = {10.1137/090771806},
eprint = {0909.4061},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Halko-et-al-2009.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
keywords = {dimension reduction,eigenvalue decomposition,interpolative decomposition,johnson,lindenstrauss lemma,matrix approximation,parallel algorithm,pass-efficient algorithm,principal component analysis,random matrix,randomized algorithm,rank-revealing qr factoriza-,singular value decomposition,streaming algorithm,tion},
pages = {1--74},
title = {{Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}},
url = {http://arxiv.org/abs/0909.4061},
year = {2009}
}
@book{Chun2006,
author = {Chun, By Wesley J and Chun, Wesley J},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Core Python Programming, Second Edition (2006).pdf:pdf},
isbn = {9780132269933},
keywords = {{\#} ISBN-10: 0132269937,{\#} ISBN-13: 978-0132269933,{\#} Language: English,{\#} Paperback: 1120 pages,{\#} Paperback: 1120 pages$\backslash$r$\backslash$n{\#} Publisher: Prentice H,{\#} Publisher: Prentice Hall PTR,2006),2006)$\backslash$r$\backslash$n{\#} Language: Engl,2nd edition (September 18},
pages = {1--1120},
title = {{Core Python Programming , Second Edition}},
year = {2006}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/Hyvarinen97.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Independent Component Analysis by Minimization of Mutual Information}},
volume = {53},
year = {2013}
}
@article{Granger2011,
author = {Granger, Brian E and Hunter, John D},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/python-an-ecosystem-for-scientific-computing.pdf:pdf},
pages = {13--21},
title = {{Python : An Ecosystem}},
year = {2011}
}
@book{Tryfos1998,
abstract = {Focusing on the principal methods for analysis and forecasting, the text is designed to fit two types of course design, the traditional approach for a technical course or the frequently used approach in business courses. Examples, exercises, problems and small and large cases are provided to fit into either or both approaches. An instructor's manual is available which includes solutions, author's notes of each case and possible alternatives, and the programs used by the author.},
author = {Gorsuch, Richard L.},
edition = {English},
editor = {{Wiley; 1 edition (January 23}, 1998)},
file = {:C$\backslash$:/Users/Dan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Example - 2001 - Chapter 14 Factor analysis.pdf:pdf},
isbn = {978-0471123842},
pages = {592},
title = {{Factor analysis}},
year = {1998}
}
@book{Rossum1996,
author = {Watters, Aaron and {Van Rossum}, Guido and Alstrom, James C.},
editor = {Books, M{\&}T},
isbn = {1558514848, 9781558514843},
pages = {477},
title = {{Internet Programming with Python}},
year = {1996}
}
@article{Stone2005,
abstract = {Given a set ofM signal mixtures (x1,x2, . . . ,xM) (e.g. microphone outputs), each of which is a different mixture of a set of M statistically independent source signals (s1, s2, . . . , sM) (e.g. voices), independent component analysis (ICA) recovers the source signals (voices) from the signal mixtures. ICA is based on the assumptions that source signals are statistically independent and that they have non-Gaussian distributions. Different physical processes usually generate statistically independent and non-Gaussian signals, so that, in the process of extracting such signals from a set of signal mixtures, ICA effectively recovers the underlying physical causes for a given set of measured signal mixtures.},
author = {Stone, James V.},
doi = {10.1002/0470013192.bsa297},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/ica{\_}encyc{\_}jvs4everrit2005.pdf:pdf},
isbn = {9780470860809},
journal = {Encyclopedia of Statistics in Behavioral Science},
keywords = {Independent Component Analysis},
number = {Novembre},
pages = {1--11},
title = {{Independent Component Analysis}},
url = {http://doi.wiley.com/10.1002/0470013192.bsa297},
volume = {2},
year = {2005}
}
@misc{R2008,
address = {Vienna},
author = {Team, R Development Core},
doi = {3-900051-07-0},
publisher = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {http://www.r-project.org},
urldate = {2017-10-23},
year = {2008}
}
@book{HarryHHarman1976,
abstract = {This thoroughly revised third edition of Harry H. Harman's authoritative text incorporates the many new advances made in computer science and technology over the last ten years. The author gives full coverage to both theoretical and applied aspects of factor analysis from its foundations through the most advanced techniques. This highly readable text will be welcomed by researchers and students working in psychology, statistics, economics, and related disciplines.},
author = {{Harry H. Harman}},
edition = {Illustrate},
editor = {{University of Chicago Press}, 1976},
isbn = {0226316521, 9780226316529},
pages = {269},
title = {{Modern Factor Analysis}},
year = {1976}
}
@article{Zhang2013,
abstract = {In automated remote sensing based image analysis, it is important to consider the multiple features of a certain pixel, such as the spectral signature, morphological property, and shape feature, in both the spatial and spectral domains, to improve the classification accuracy. Therefore, it is essential to consider the complementary properties of the different features and combine them in order to obtain an accurate classification rate. In this paper, we introduce a modified stochastic neighbor embedding (MSNE) algorithm for multiple features dimension reduction (DR) under a probability preserving projection framework. For each feature, a probability distribution is constructed based on t-distributed stochastic neighbor embedding (. t-SNE), and we then alternately solve t-SNE and learn the optimal combination coefficients for different features in the proposed multiple features DR optimization. Compared with conventional remote sensing image DR strategies, the suggested algorithm utilizes both the spatial and spectral features of a pixel to achieve a physically meaningful low-dimensional feature representation for the subsequent classification, by automatically learning a combination coefficient for each feature. The classification results using hyperspectral remote sensing images (HSI) show that MSNE can effectively improve RS image classification performance. {\textcopyright} 2013 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Zhang, Lefei and Zhang, Liangpei and Tao, Dacheng and Huang, Xin},
doi = {10.1016/j.isprsjprs.2013.05.009},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/A modified stochastic neighbor embedding for multi-feature dimension.pdf:pdf},
isbn = {0924-2716},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Classification,Dimension reduction,Hyperspectral image,Multiple features,Stochastic neighbor embedding},
pages = {30--39},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{A modified stochastic neighbor embedding for multi-feature dimension reduction of remote sensing images}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2013.05.009},
volume = {83},
year = {2013}
}
@article{Hyvarinen2000,
abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of nongaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
archivePrefix = {arXiv},
arxivId = {1504.05070},
author = {Hyv{\"{a}}rinen, Aapo and Oja, Erkki},
doi = {10.1016/S0893-6080(00)00026-5},
eprint = {1504.05070},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/ICA{\_}Hyvarinen.pdf:pdf},
isbn = {3589451327},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {Independent component analysis,blind signal separation,factor analysis,projection pursuit,representation,source separation},
number = {45},
pages = {411--430},
pmid = {10946390},
title = {{Independent Component Analysis: Algorithms and Applications}},
volume = {13},
year = {2000}
}
@article{Walt2011,
author = {Walt, Stefan Van Der and Colbert, S Chris and Varoquaux, Ga{\"{e}}l and Walt, Stefan Van Der and Colbert, S Chris and Varoquaux, Ga{\"{e}}l and Numpy, The},
doi = {10.1109/MCSE.2011.37},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/numpy{\_}final.pdf:pdf},
title = {{The NumPy array: a structure for efficient numerical computation}},
url = {https://hal.inria.fr/inria-00564007/document},
year = {2011}
}
@article{Maaten2008,
abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare itwith many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {Maaten, Laurens Van Der and Hinton, Geoffrey},
doi = {10.1007/s10479-011-0841-3},
eprint = {1307.1662},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/visualizing data using tsne IMPORTANTE.pdf:pdf},
isbn = {1532-4435},
issn = {1940-6029},
journal = {Journal of Machine Learning Research 1},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
number = {1},
pages = {267--84},
pmid = {20652508},
title = {{Visualizing Data using t-SNE}},
url = {http://link.springer.com/10.1007/s10479-011-0841-3{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/20652508},
volume = {620},
year = {2008}
}
@article{Soulie1987,
author = {Soulie, Fogelman and Gallinari, F. and P. and Lecun, Yann and Thiria, S.},
journal = {Automata networks in computer science, theory and applications},
pages = {133--186},
title = {{Automata networks and artificial intelligence}},
year = {1987}
}
@article{Guyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/an{\_}introduction{\_}to{\_}variable{\_}selection.pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Pet2010,
author = {Petˇ, M},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/WDS10{\_}113{\_}i2{\_}Petricek.pdf:pdf},
isbn = {9788073781392},
pages = {82--87},
title = {{Components in Data Analysis}},
volume = {1},
year = {2010}
}
@article{VanDerMaaten2009,
abstract = {In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear tech- niques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identi- fying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.},
author = {{Van Der Maaten}, Laurens and Postma, Eric and {Van Den Herik}, Jaap},
doi = {10.1080/13506280444000102},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/dimensionality{\_}reduction{\_}a{\_}comparative{\_}review.pdf:pdf},
issn = {0169328X},
journal = {October},
pages = {1--35},
pmid = {7877450},
title = {{Dimensionality Reduction : A Comparative Review}},
url = {http://www.uvt.nl/ticc},
year = {2009}
}
@article{Tucker1951,
author = {Tucker, Ledyard R.},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/tucker-19514.pdf:pdf},
journal = {Personnel Research Section Report},
title = {{A method for synthesis of factor analysis studies}},
volume = {984},
year = {1951}
}
@article{Burges2010,
abstract = {We give a tutorial overview of several foundational methods for dimension reduction. We divide the methods into projective methods and methods that model the manifold on which the data lies. For projective methods, we review projection pursuit, principal component analysis (PCA), kernel PCA, probabilistic PCA, canonical correlation analysis (CCA), kernel CCA, Fisher discriminant analysis, oriented PCA, and several techniques for sufficient dimension reduction. For the manifold methods, we review multidimensional scaling (MDS), landmark MDS, Isomap, locally linear embedding, Laplacian eigenmaps, and spectral clustering. Although the review focuses on foundations, we also provide pointers to some more modern techniques. We also describe the correlation dimension as one method for estimating the intrinsic dimension, and we point out that the notion of dimension can be a scale-dependent quantity. The Nystr{\"{o}}m method, which links several of the manifold algorithms, is also reviewed. We use a publicly available dataset to illustrate some of the methods. The goal is to provide a self-contained overview of key concepts underlying many of these algorithms, and to give pointers for further reading.},
author = {Burges, Cjc},
doi = {10.1561/2200000002},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/dimension{\_}reduction{\_}a{\_}guided{\_}tour.pdf:pdf},
isbn = {9781601983787},
issn = {19358237},
journal = {Foundations and Trends in Machine Learning},
number = {4},
pages = {275--365},
title = {{Dimension reduction: A guided tour}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL{\&}doi=2200000002{\%}5Cnhttp://research.microsoft.com/pubs/150728/FnT{\_}dimensionReduction.pdf},
volume = {2},
year = {2010}
}
@misc{chollet2015,
author = {et al Chollet, Fran{\c{c}}ois},
booktitle = {GitHub},
title = {{Keras}},
url = {https://keras.io/getting-started/faq/},
urldate = {2017-10-24},
year = {2015}
}
@article{Nosrati2011,
abstract = {in this paper, we are going to represent an introduction to Python programming language and prove it as a suitable language for both learning and real world programming. Due to this, we will begin with philosophy and history of this language, and then get into its features. Program types will be investigated as the next step, and then alternatives and complements of Python which are some useful packages and modules will be introduced. As the last part, some of important software and websites that are written in Python will be introduces, to be a real world evidence for Python.},
author = {Nosrati, Masoud},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/python{\_}an{\_}appropriate{\_}language{\_}for{\_}real{\_}world{\_}programming.pdf:pdf},
journal = {World Applied Programming},
keywords = {Key word},
number = {12},
pages = {110--117},
title = {{Python: An appropriate language for real world programming}},
year = {2011}
}
@article{Hinton2002,
abstract = {Abstract We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high- ... $\backslash$n},
author = {Hinton, Geoffrey E and Roweis, Sam T},
doi = {http://books.nips.cc/papers/files/nips15/AA45.pdf},
file = {:D$\backslash$:/MU{\_}Bioinformatica/TFM/PROPOSAL/Materiales/2276-stochastic-neighbor-embedding.pdf:pdf},
isbn = {0262025507},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {833--840},
title = {{Stochastic neighbor embedding}},
year = {2002}
}
